Automatically generated by Mendeley Desktop 1.16.2-dev5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{StephenMerity2016,
author = {{Stephen Merity}},
booktitle = {http://smerity.com/},
title = {{In deep learning, architecture engineering is the new feature engineering}},
url = {http://smerity.com/articles/2016/architectures{\_}are{\_}the{\_}new{\_}feature{\_}engineering.html},
urldate = {2016-09-01},
year = {2016}
}
@article{disfa,
abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
doi = {10.1109/T-AFFC.2013.4},
file = {:home/luka/Documents/Mendeley/Mavadati et al/Mavadati et al. - 2013 - DISFA A Spontaneous Facial Action Intensity Database.pdf:pdf},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Trans. Affect. Comput.},
keywords = {FACS,action units,facial expression,intensity,spontaneous facial behavior,video corpus},
month = {apr},
number = {2},
pages = {151--160},
publisher = {IEEE},
title = {{DISFA: A spontaneous facial action intensity database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6475933},
volume = {4},
year = {2013}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/luka/Documents/Mendeley/Krizhevsky, Sutskever, Hinton/Krizhevsky, Sutskever, Hinton - Unknown - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@incollection{JoshuaM.Susskind123,
abstract = {This book provides an overview of state of the art research in Affective Computing. It presents new ideas, original results and practical experiences in this increasingly important research field. The book consists of 23 chapters categorized into four sections. Since one of the most important means of human communication is facial expression, the first section of this book (Chapters 1 to 7) presents a research on synthesis and recognition of facial expressions. Given that we not only use the face but also body movements to express ourselves, in the second section (Chapters 8 to 11) we present a research on perception and generation of emotional expressions by using full-body motions. The third section of the book (Chapters 12 to 16) presents computational models on emotion, as well as findings from neuroscience research. In the last section of the book (Chapters 17 to 22) we present applications related to affective computing.},
author = {{Joshua M. Susskind}, Geoffrey E. Hinton and Anderson, Javier R. Movellan and Adam K.},
booktitle = {Affect. Comput.},
chapter = {23},
doi = {10.5772/56897},
editor = {Or, Jimmy},
file = {:home/luka/Documents/Mendeley/Joshua M. Susskind, Anderson/joshfacechapter.pdf:pdf},
isbn = {978-3-902613-23-3},
pages = {452},
publisher = {I-Tech Education and Publishing},
title = {{Generating Facial Expressions with Deep Belief Nets}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/joshfacechapter.pdf},
year = {2008}
}
@article{mnist,
abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.},
author = {LeCun, Y. and Cortes, C. and Burges, C. J. C.},
isbn = {7842500200015},
title = {{The MNIST database of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {1998}
}
@article{Osendorfer2014,
abstract = {We present a computationally efficient architecture for image super-resolution that achieves state-of-the-art results on imageswith large spatial extend. Apart from utilizing Convolutional Neural Networks, our approach leverages recent advances in fast approximate inference for sparse coding.We empirically show that upsampling methods work much better on latent representations than in the original spatial domain. Our experiments indicate that the proposed architecture can serve as a basis for additional future improvements in image super-resolution.},
author = {Osendorfer, Christian and Soyer, Hubert and van der Smagt, Patrick},
doi = {10.1007/978-3-319-12643-2_31},
file = {:home/luka/Documents/Mendeley/Osendorfer, Soyer, van der Smagt/Osendorfer, Soyer, van der Smagt - 2014 - Image Super-Resolution with Fast Approximate Convolutional Sparse Coding.pdf:pdf},
isbn = {9783319126425},
issn = {16113349},
keywords = {convolutional neural,image processing,sparse coding},
pages = {250--257},
publisher = {Springer International Publishing},
title = {{Image Super-Resolution with Fast Approximate Convolutional Sparse Coding}},
url = {http://link.springer.com/10.1007/978-3-319-12643-2{\_}31},
year = {2014}
}
@book{tfd,
author = {{J. M. Susskind, A. K. Anderson}, and G. E. Hinton},
publisher = {Department of Computer Science, University of Toronto, Toronto},
title = {{The Toronto Face Database}},
year = {2010}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:home/luka/Documents/Mendeley/Springenberg et al/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf:pdf},
isbn = {9781600066634},
journal = {Iclr},
month = {dec},
pages = {1--14},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2015}
}
@article{Eleftheriadis2016,
abstract = {We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation.},
archivePrefix = {arXiv},
arxivId = {1604.02917},
author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Deisenroth, Marc P. and Pantic, Maja},
eprint = {1604.02917},
file = {:home/luka/Documents/Mendeley/Eleftheriadis et al/Eleftheriadis et al. - 2016 - Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis.pdf:pdf},
month = {apr},
title = {{Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis}},
url = {http://arxiv.org/abs/1604.02917},
year = {2016}
}
@inproceedings{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/luka/Documents/Mendeley/Girshick et al/Girshick et al. - 2013 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
month = {nov},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@article{precogbook,
abstract = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics. An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
author = {{Duda O.}, Richard and {Hart E.}, Peter and {Stork G.}, David},
doi = {10.1007/BF01237942},
isbn = {978-0-471-05669-0},
issn = {0176-4268},
journal = {Zhurnal Eksp. i Teor. Fiz.},
pages = {680},
pmid = {2630878},
publisher = {Wiley},
title = {{Pattern Classification}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0$\backslash$nhttp://www.ai.mit.edu/courses/6.891-f00/text/DHSAppendix.pdf},
year = {2000}
}
@article{Savran2012,
abstract = {a b s t r a c t Facial Action Coding System (FACS) is the de facto standard in the analysis of facial expressions. FACS de-scribes expressions in terms of the configuration and strength of atomic units called Action Units: AUs. FACS defines 44 AUs and each AU intensity is defined on a nonlinear scale of five grades. There has been sig-nificant progress in the literature on the detection of AUs. However, the companion problem of estimating the AU strengths has not been much investigated. In this work we propose a novel AU intensity estimation scheme applied to 2D luminance and/or 3D surface geometry images. Our scheme is based on regression of selected image features. These features are either non-specific, that is, those inherited from the AU detection algorithm, or are specific in that they are selected for the sole purpose of intensity estimation. For thorough-ness, various types of local 3D shape indicators have been considered, such as mean curvature, Gaussian cur-vature, shape index and curvedness, as well as their fusion. The feature selection from the initial plethora of Gabor moments is instrumented via a regression that optimizes the AU intensity predictions. Our AU inten-sity estimator is person-independent and when tested on 25 AUs that appear singly or in various combina-tions, it performs significantly better than the state-of-the-art method which is based on the margins of SVMs designed for AU detection. When evaluated comparatively, one can see that the 2D and 3D modalities have relative merits per upper face and lower face AUs, respectively, and that there is an overall improve-ment if 2D and 3D intensity estimations are used in fusion.},
author = {Savran, Arman and Sankur, Bulent and {Taha Bilge}, M},
doi = {10.1016/j.imavis.2011.11.008},
file = {:home/luka/Documents/Mendeley/Savran, Sankur, Taha Bilge/Savran, Sankur, Taha Bilge - 2012 - Regression-based intensity estimation of facial action units ☆.pdf:pdf},
journal = {IMAVIS},
keywords = {3D facial expression recognition,Action unit intensity estimation,AdaBoost.RT,Facial Action Coding System,Feature selection,SVM regression},
pages = {774--784},
title = {{Regression-based intensity estimation of facial action units ☆}},
volume = {30},
year = {2012}
}
@article{autong,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:home/luka/Documents/Mendeley/Ng/Andrew Ng - 2011 - CS294A Lecture notes Sparse autoencoder.pdf:pdf},
institution = {Stanford},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lect. notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/luka/Documents/Mendeley/Kingma, Ba/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
booktitle = {2014 IEEE Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.2014.220},
file = {:home/luka/Documents/Mendeley/Taigman et al/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
isbn = {978-1-4799-5118-5},
keywords = {3D face modeling,Agriculture,DeepFace,Face,Face recognition,LFW dataset,Shape,Solid modeling,Three-dimensional displays,Training,alignment step,deep neural network,face recognition,face representation,face verification,human-level performance,image representation,labeled faces in the wild,neural nets,piecewise affine transformation,representation step},
month = {jun},
pages = {1701--1708},
publisher = {IEEE},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909616},
year = {2014}
}
@misc{AndrewGibiansky,
abstract = {Next, let's figure out how to do the exact same thing for convolutional neural networks. While the mathematical theory should be exactly the same, the actual derivation will be slightly more complex due to the architecture of convolutional neural networks.},
author = {{Andrew Gibiansky}},
booktitle = {2014},
title = {{Convolutional Neural Networks}},
url = {http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/},
urldate = {2016-06-10}
}
@article{Corneanu2016,
abstract = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
author = {Corneanu, Ciprian A and Oliu, Marc and Cohn, Jeffrey F and Escalera, Sergio},
doi = {10.1109/TPAMI.2016.2515606},
issn = {1939-3539},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {jan},
pmid = {26761193},
title = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-related Applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26761193},
year = {2016}
}
@article{Lucey2010,
abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this pe-riod, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algo-rithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22{\%} and the number of subjects by 27{\%}. The target expres-sion for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a lin-ear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
author = {Lucey, Patrick and Cohn, Jeffrey F and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
file = {:home/luka/Documents/Mendeley/Lucey et al/Lucey et al. - Unknown - The Extended Cohn-Kanade Dataset (CK) A complete dataset for action unit and emotion-specified expression.pdf:pdf},
isbn = {9781424470303},
journal = {IEEE Conf. Comput. Vis. Pattern Recognit. Work.},
number = {July},
pages = {94--101},
title = {{The extended cohn-kande dataset (CK+): A complete facial expression dataset for action unit and emotionspeciﬁed expression}},
year = {2010}
}
@inproceedings{Valstar,
abstract = {Despite efforts towards evaluation standards in facial expression analysis (e.g. FERA 2011), there is a need for up-to-date standardised evaluation procedures, focusing in particular on current challenges in the field. One of the challenges that is actively being addressed is the automatic estimation of expression intensities. To continue to provide a standardisation platform and to help the field progress beyond its current limitations, the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015) will challenge participants to estimate FACS Action Unit (AU) intensity as well as AU occurrence on a common benchmark dataset with reliable manual annotations. Evaluation will be done using a clear and well-defined protocol. In this paper we present the second such challenge in automatic recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on Face and Gesture Recognition, May 2015, in Ljubljana, Slovenia. Three sub-challenges are defined: the detection of AU occurrence, the estimation of AU intensity for pre-segmented data, and fully automatic AU intensity estimation. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for the three sub-challenges.},
author = {Valstar, Michel F and Almaev, Timur and Girard, Jeffrey M and McKeown, Gary and Mehu, Marc and Yin, Lijun and Pantic, Maja and Cohn, Jeffrey F},
booktitle = {2015 11th IEEE Int. Conf. Work. Autom. Face Gesture Recognit.},
doi = {10.1109/FG.2015.7284874},
file = {:home/luka/Documents/Mendeley/Valstar et al/Valstar et al. - Unknown - FERA 2015 -Second Facial Expression Recognition and Analysis Challenge.pdf:pdf},
isbn = {978-1-4799-6026-2},
keywords = {AU intensity,AU occurrence,Databases,Estimation,FACS action unit,FERA 2015,Face recognition,Feature extraction,Gold,Reliability,Training,emotion recognition,expression intensities estimation,face recognition,facial expression analysis,facial expression recognition and analysis challen,standardisation platform},
pages = {1--8},
title = {{FERA 2015 - second Facial Expression Recognition and Analysis challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7284874},
volume = {06},
year = {2015}
}
@article{stacks,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
file = {:home/luka/Documents/Mendeley/Vincent et al/Vincent et al. - 2010 - Stacked Denoising Autoencoders Learning Useful Representations in a Deep Network with a Local Denoising Criterio.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
number = {3},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@article{Glauner2015,
abstract = {This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45{\%} is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55{\%} to 79.67{\%}. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.},
archivePrefix = {arXiv},
arxivId = {1508.06535},
author = {Glauner, Patrick O.},
eprint = {1508.06535},
file = {:home/luka/Documents/Mendeley/Glauner/Glauner - 2015 - Deep Convolutional Neural Networks for Smile Recognition.pdf:pdf},
month = {aug},
title = {{Deep Convolutional Neural Networks for Smile Recognition}},
url = {http://arxiv.org/abs/1508.06535},
year = {2015}
}

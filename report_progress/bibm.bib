@article{Gudi2015,
abstract = {— Ground truth annotation of the occurrence and intensity of FACS Action Unit (AU) activation requires great amount of attention. The efforts towards achieving a common platform for AU evaluation have been addressed in the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015). Participants are invited to estimate AU occurrence and intensity on a common benchmark dataset. Conventional approaches towards achieving automated methods are to train multiclass classifiers or to use regression models. In this paper, we propose a novel application of a deep convolutional neural network (CNN) to recognize AUs as part of FERA 2015 challenge. The 7 layer network is composed of 3 convolutional layers and a max-pooling layer. The final fully connected layers provide the classification output. For the selected tasks of the challenge, we have trained two different networks for the two different datasets, where one focuses on the AU occurrences and the other on both occurrences and intensities of the AUs. The occurrence and intensity of AU activation are estimated using specific neuron activations of the output layer. This way, we are able to create a single network architecture that could simultaneously be trained to produce binary and continuous classification output.},
author = {Gudi, Amogh and Tasli, H Emrah and den Uyl, Tim M and Maroulis, Andreas},
file = {:home/luka/Desktop/07284873.pdf:pdf},
isbn = {9781479960262},
journal = {11th IEEE International Conference on Automatic Face and Gesture Recognitions (FG 2015), FERA 2015 Challenge},
title = {{Deep Learning based FACS Action Unit Occurrence and Intensity Estimation}},
volume = {2013},
year = {2015}
}
@misc{Jaiswal2016,
abstract = {Spontaneous facial expression recognition under uncontrolled conditions is a hard task. It depends on multiple factors including shape, appearance and dynamics of the facial features, all of which are adversely affected by environmental noise and low intensity signals typical of such conditions. In this work, we present a novel approach to Facial Action Unit detection using a combination of Convolutional and Bi-directional Long Short-Term Memory Neural Networks (CNN-BLSTM), which jointly learns shape, appearance and dynamics in a deep learning manner. In addition, we introduce a novel way to encode shape features using binary image masks computed from the locations of facial landmarks. We show that the combination of dynamic CNN features and Bi-directional Long Short-Term Memory excels at modelling the temporal information. We thoroughly evaluate the contributions of each component in our system and show that it achieves state-of-the-art performance on the FERA-2015 Challenge dataset.},
author = {Jaiswal, Shashank and Valstar, Michel F.},
language = {en},
month = {jan},
title = {{Deep learning the dynamic appearance and shape of facial action units}},
url = {http://eprints.nottingham.ac.uk/31301/1/paper.pdf},
year = {2016}
}
@misc{MartnAbadiAshishAgarwalPaulBarhamEugeneBrevdoZhifengChen,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
annote = {Large-Scale Machine Learning on Heterogeneous Distributed Systems},
author = {{Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen}, Craig Citro and {Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat}, Ian Goodfellow and {Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz}, Lukasz Kaiser and {Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray}, ´ and {Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever}, Kunal Talwar and {Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals}, ´ and {Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu}, and Xiaoqiang Zheng},
keywords = {Google,software,tensorflow,whitepaper},
mendeley-tags = {Google,software,tensorflow,whitepaper},
title = {{TensorFlow White Paper}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf}
}

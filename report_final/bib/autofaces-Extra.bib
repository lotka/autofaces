Automatically generated by Mendeley Desktop 1.17-dev2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
file = {:home/luka/Dropbox/Mendeley/Kingma, Welling/2013/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@article{ThibaudSenechal2016,
author = {{Thibaud Senechal}, Daniel McDuff and Rana el Kaliouby},
file = {:home/luka/Dropbox/Mendeley/Thibaud Senechal/2016/Facial-Action-Unit-Detection-using-Active-Learning-and-an-Efficient-Non-Linear-Kernel-Approximation.pdf:pdf},
journal = {Affectiva},
title = {{Facial Action Unit Detection using Active Learning and an Efficient Non-Linear Kernel Approximation}},
url = {http://www.affectiva.com/wp-content/uploads/2016/02/Facial-Action-Unit-Detection-using-Active-Learning-and-an-Efficient-Non-Linear-Kernel-Approximation.pdf},
year = {2016}
}
@book{ThomasDietterich,
abstract = {During the last years, semi-supervised learning has emerged as an exciting new direction in machine learning reseach. It is closely related to profound issues of how to do inference from data, as witnessed by its overlap with transductive inference (the distinctions are yet to be made precise).},
author = {Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
title = {{Semi-Supervised Learning}},
url = {http://www.acad.bg/ebook/ml/MITPress- SemiSupervised Learning.pdf}
}
@article{Dosovitskiy2015,
abstract = {We describe an approach to incorporate diversity into spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the {\$}F{\_}1{\$} score of 90.18, and for German we achieve the {\$}F{\_}1{\$} score of 83.38.},
archivePrefix = {arXiv},
arxivId = {1506.0275},
author = {Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-10593-2_13},
eprint = {1506.0275},
file = {:home/luka/Dropbox/Mendeley/Dosovitskiy, Brox/2015/Dosovitskiy, Brox - 2015 - Inverting Convolutional Networks with Convolutional Networks.pdf:pdf},
isbn = {9781941643327},
journal = {arXiv Prepr. arXiv1506.02753},
month = {jun},
pages = {1--15},
title = {{Inverting Convolutional Networks with Convolutional Networks}},
url = {http://arxiv.org/abs/1506.02753 http://arxiv.org/abs/1506.0275},
year = {2015}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04623v1},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
eprint = {arXiv:1502.04623v1},
file = {:home/luka/Dropbox/Mendeley/Gregor et al/2014/Gregor et al. - 2014 - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
month = {feb},
pages = {1--16},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2014}
}
@article{Rocki2016,
abstract = {There exists a theory of a single general-purpose learning algorithm which could explain the principles of its operation. This theory assumes that the brain has some initial rough architecture, a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from both architectural and functional perspectives.},
archivePrefix = {arXiv},
arxivId = {1603.08262},
author = {Rocki, Kamil},
eprint = {1603.08262},
file = {:home/luka/Dropbox/Mendeley/Rocki/2016/Rocki - 2016 - Towards Machine Intelligence.pdf:pdf},
journal = {IBM},
month = {mar},
title = {{Towards Machine Intelligence}},
url = {http://arxiv.org/abs/1603.08262},
year = {2016}
}

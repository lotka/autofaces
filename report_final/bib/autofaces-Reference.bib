Automatically generated by Mendeley Desktop 1.17-dev2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Zhou2014,
abstract = {Traditionally, when generative models of data are developed via deep architectures, greedy layer-wise pre-training is employed. In a well-trained model, the lower layer of the architecture models the data distribution conditional upon the hidden variables, while the higher layers model the hidden distribution prior. But due to the greedy scheme of the layerwise training technique, the parameters of lower layers are fixed when training higher layers. This makes it extremely challenging for the model to learn the hidden distribution prior, which in turn leads to a suboptimal model for the data distribution. We therefore investigate joint training of deep autoencoders, where the architecture is viewed as one stack of two or more single-layer autoencoders. A single global reconstruction objective is jointly optimized, such that the objective for the single autoencoders at each layer acts as a local, layer-level regularizer. We empirically evaluate the performance of this joint training scheme and observe that it not only learns a better data model, but also learns better higher layer representations, which highlights its potential for unsupervised feature learning. In addition, we find that the usage of regularizations in the joint training scheme is crucial in achieving good performance. In the supervised setting, joint training also shows superior performance when training deeper models. The joint training framework can thus provide a platform for investigating more efficient usage of different types of regularizers, especially in light of the growing volumes of available unlabeled data.},
archivePrefix = {arXiv},
arxivId = {1405.1380},
author = {Zhou, Yingbo and Arpit, Devansh and Nwogu, Ifeoma and Govindaraju, Venu},
eprint = {1405.1380},
file = {:home/luka/Dropbox/Mendeley/Zhou et al/2014/Zhou et al. - Unknown - Is Joint Training Better for Deep Auto-Encoders.pdf:pdf},
pages = {1--11},
title = {{Is Joint Training Better for Deep Auto-Encoders?}},
url = {http://arxiv.org/abs/1405.1380},
year = {2014}
}
@article{precogbook,
abstract = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics. An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
author = {{Duda O.}, Richard and {Hart E.}, Peter and {Stork G.}, David},
doi = {10.1007/BF01237942},
isbn = {978-0-471-05669-0},
issn = {0176-4268},
journal = {Zhurnal Eksp. i Teor. Fiz.},
pages = {680},
pmid = {2630878},
publisher = {Wiley},
title = {{Pattern Classification}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0{\%}5Cnhttp://www.ai.mit.edu/courses/6.891-f00/text/DHSAppendix.pdf},
year = {2000}
}
@article{stacks,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
file = {:home/luka/Dropbox/Mendeley/Vincent et al/2010/Vincent et al. - 2010 - Stacked Denoising Autoencoders Learning Useful Representations in a Deep Network with a Local Denoising Criterio.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
number = {3},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/luka/Dropbox/Mendeley/Srivastava et al/2014/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {deep learning,dropout,model combination,neural networks,regularization},
mendeley-tags = {dropout},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Glauner2015,
abstract = {This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45{\%} is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55{\%} to 79.67{\%}. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.},
archivePrefix = {arXiv},
arxivId = {1508.06535},
author = {Glauner, Patrick O.},
eprint = {1508.06535},
file = {:home/luka/Dropbox/Mendeley/Glauner/2015/Glauner - 2015 - Deep Convolutional Neural Networks for Smile Recognition.pdf:pdf},
month = {aug},
title = {{Deep Convolutional Neural Networks for Smile Recognition}},
url = {http://arxiv.org/abs/1508.06535},
year = {2015}
}
@article{disfa,
abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
doi = {10.1109/T-AFFC.2013.4},
file = {:home/luka/Dropbox/Mendeley/Mavadati et al/2013/Mavadati et al. - 2013 - DISFA A spontaneous facial action intensity database.pdf:pdf},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Trans. Affect. Comput.},
keywords = {FACS,action units,facial expression,intensity,spontaneous facial behavior,video corpus},
month = {apr},
number = {2},
pages = {151--160},
publisher = {IEEE},
title = {{DISFA: A spontaneous facial action intensity database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6475933},
volume = {4},
year = {2013}
}
@book{Duchenne1867,
abstract = {Translation of: Mécanisme de la physionomie humaine.},
author = {Duchenne, G.B.},
isbn = {0521363926},
pages = {288},
publisher = {Cambridge University Press},
title = {{The Mechanism of Human Facial Expression}},
year = {1867}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:home/luka/Dropbox/Mendeley/Springenberg et al/2015/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf:pdf},
isbn = {9781600066634},
journal = {Iclr},
month = {dec},
pages = {1--14},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2015}
}
@article{Osendorfer2014,
abstract = {We present a computationally efficient architecture for image super-resolution that achieves state-of-the-art results on imageswith large spatial extend. Apart from utilizing Convolutional Neural Networks, our approach leverages recent advances in fast approximate inference for sparse coding.We empirically show that upsampling methods work much better on latent representations than in the original spatial domain. Our experiments indicate that the proposed architecture can serve as a basis for additional future improvements in image super-resolution.},
author = {Osendorfer, Christian and Soyer, Hubert and van der Smagt, Patrick},
doi = {10.1007/978-3-319-12643-2_31},
file = {:home/luka/Dropbox/Mendeley/Osendorfer, Soyer, van der Smagt/2014/Osendorfer, Soyer, van der Smagt - 2014 - Image Super-Resolution with Fast Approximate Convolutional Sparse Coding.pdf:pdf},
isbn = {9783319126425},
issn = {16113349},
keywords = {convolutional neural,image processing,sparse coding},
pages = {250--257},
publisher = {Springer International Publishing},
title = {{Image Super-Resolution with Fast Approximate Convolutional Sparse Coding}},
url = {http://link.springer.com/10.1007/978-3-319-12643-2{\_}31},
year = {2014}
}
@book{Darwin1872,
abstract = {3rd ed. General principles of expression -- General principles of expression : continued -- General principles of expression : concluded -- Means of expression in animals -- Special expressions of animals -- Special expressions of man: suffering and weeping -- Low spirits, anxiety, grief, dejection, despair -- Joy, high spirits, love, tender feelings, devotion -- Reflection -- meditation -- ill temper -- sulkiness -- determination -- Hatred and anger -- Disdain -- contempt -- disgust -- guilt -- pride, etc. -- helplessness -- patience -- affirmation and negation -- Surprise -- astonishment -- fear -- horror -- Self-attention -- shame -- shyness--modesty : blushing -- Concluding remarks and summary -- Afterword / Paul Ekman -- Appendix I : Charles Darwin's obituary / T.H. Huxley -- Appendix II : changes to the text / Paul Ekman -- Appendix III : photography and the expression of the emotions / Phillip Prodger -- Appendix IV : a note on the orientation of the plates / Phillip Prodger {\&} Paul Ekman -- Appendix V : concordance of illustrations / Phillip Prodger -- Appendix VI : list of head words from the index to the first edition -- Notes -- Notes to the commentaries.},
author = {Darwin, Charles and Cummings, Martin Marc and Duchenne, G.-B. and {John Murray (Firm)}},
isbn = {0195158067},
pages = {472},
publisher = {Oxford University Press},
title = {{The expression of the emotions in man and animals}},
year = {1872}
}
@misc{AndrewGibiansky,
abstract = {Next, let's figure out how to do the exact same thing for convolutional neural networks. While the mathematical theory should be exactly the same, the actual derivation will be slightly more complex due to the architecture of convolutional neural networks.},
author = {{Andrew Gibiansky}},
booktitle = {2014},
title = {{Convolutional Neural Networks}},
url = {http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/},
urldate = {2016-06-10}
}
@article{OReilly2013,
abstract = {How does the brain learn to recognize objects visually, and perform this difficult feat robustly in the face of many sources of ambiguity and variability? We present a computational model based on the biology of the relevant visual pathways that learns to reliably recognize 100 different object categories in the face of naturally occurring variability in location, rotation, size, and lighting. The model exhibits robustness to highly ambiguous, partially occluded inputs. Both the unified, biologically plausible learning mechanism and the robustness to occlusion derive from the role that recurrent connectivity and recurrent processing mechanisms play in the model. Furthermore, this interaction of recurrent connectivity and learning predicts that high-level visual representations should be shaped by error signals from nearby, associated brain areas over the course of visual learning. Consistent with this prediction, we show how semantic knowledge about object categories changes the nature of their learned visual representations, as well as how this representational shift supports the mapping between perceptual and conceptual knowledge. Altogether, these findings support the potential importance of ongoing recurrent processing throughout the brain's visual system and suggest ways in which object recognition can be understood in terms of interactions within and between processes over time.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {O'Reilly, Randall C. and Wyatte, Dean and Herd, Seth and Mingus, Brian and Jilk, David J.},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/luka/Dropbox/Mendeley/O'Reilly et al/2013/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {1664-1078},
issn = {16641078},
journal = {Front. Psychol.},
keywords = {Computational model,Feedback,Object recognition,Recurrent processing,Winners-take-all mechanism},
month = {dec},
number = {APR},
pmid = {23554596},
title = {{Recurrent processing during object recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {4},
year = {2013}
}
@article{Kaltwang2015,
abstract = {—Certain inner feelings and physiological states like pain are subjective states that cannot be directly measured, but can be estimated from spontaneous facial expressions. Since they are typically characterized by subtle movements of facial parts, analysis of the facial details is required. To this end, we formulate a new regression method for continuous estimation of the intensity of facial behavior interpretation, called Doubly Sparse Relevance Vector Machine (DSRVM). DSRVM enforces double sparsity by jointly selecting the most relevant training examples (a.k.a. relevance vectors) and the most important kernels associated with facial parts relevant for interpretation of observed facial expressions. This advances prior work on multi-kernel learning, where sparsity of relevant kernels is typically ignored. Empirical evaluation on challenging Shoulder Pain videos, and the benchmark DISFA and SEMAINE datasets demonstrate that DSRVM outperforms competing approaches with a multi-fold reduction of running times in training and testing.},
author = {Kaltwang, Sebastian and Todorovic, Sinisa and Pantic, Maja},
doi = {10.1109/TPAMI.2015.2501824},
file = {:home/luka/Dropbox/Mendeley/Kaltwang, Todorovic, Pantic/2015/Kaltwang, Todorovic, Pantic - 2015 - Doubly Sparse Relevance Vector Machine for Continuous Facial Behavior Estimation.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Estimation,Face,Facial expressions,Kernel,Multiple Kernel Learning,Pain,Regression,Relevance Vector Machine,Support vector machines,Training,Videos},
number = {0},
pages = {1--1},
title = {{Doubly Sparse Relevance Vector Machine for Continuous Facial Behavior Estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7331650},
volume = {0},
year = {2015}
}
@article{Bahrampour2016,
abstract = {In the last decade, rapid growth in mobile applications, web technologies, social media generating unstructured data has led to the advent of various nosql data stores. Demands of web scale are in increasing trend everyday and nosql databases are evolving to meet up with stern big data requirements. The purpose of this paper is to explore nosql technologies and present a comparative study of document and column store nosql databases such as cassandra, MongoDB and Hbase in various attributes of relational and distributed database system principles. Detailed study and analysis of architecture and internal working cassandra, Mongo DB and HBase is done theoretically and core concepts are depicted. This paper also presents evaluation of cassandra for an industry specific use case and results are published.},
archivePrefix = {arXiv},
arxivId = {1511.06435},
author = {Reis, Cassius V C},
doi = {10.1227/01.NEU.0000297044.82035.57},
eprint = {1511.06435},
file = {:home/luka/Dropbox/Mendeley/Reis/2015/Bahrampour et al. - 2015 - Comparative Study of Deep Learning Software Frameworks.pdf:pdf},
isbn = {9150617397},
issn = {15760162},
journal = {Neurosurgery},
keywords = {cerebral localization,craniocerebral topography,neurosurgical history,neurosurgical navi-},
number = {2},
pages = {294--310},
title = {{C Omparative S Tudy of C Ranial T Opographic}},
url = {http://search.proquest.com/docview/1626785137?accountid=10755{\%}5Cnhttp://sfx.bib-bvb.de/sfx{\_}uben?url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}genre=unknown{\&}sid=ProQ:ProQ:abiglobal{\&}atitle=BEHAVIOUR+OF+ELDERLY+USERS+ON+FACEBOOK+TOWARD+VIRAL+M},
volume = {62},
year = {2008}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, Richard and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:home/luka/Dropbox/Mendeley/Deng et al/2009/Deng et al. - 2009 - ImageNet A large-scale hierarchical image database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conf. Comput. Vis. Pattern Recognit.},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}
@article{mnist,
abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.},
author = {LeCun, Y. and Cortes, C. and Burges, C. J. C.},
isbn = {7842500200015},
title = {{The MNIST database of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {1998}
}
@article{Eleftheriadis2016,
abstract = {We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation.},
archivePrefix = {arXiv},
arxivId = {1604.02917},
author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Deisenroth, Marc P. and Pantic, Maja},
eprint = {1604.02917},
file = {:home/luka/Dropbox/Mendeley/Eleftheriadis et al/2016/Eleftheriadis et al. - 2016 - Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis.pdf:pdf},
month = {apr},
title = {{Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis}},
url = {http://arxiv.org/abs/1604.02917},
year = {2016}
}
@article{Savran2012,
abstract = {a b s t r a c t Facial Action Coding System (FACS) is the de facto standard in the analysis of facial expressions. FACS de-scribes expressions in terms of the configuration and strength of atomic units called Action Units: AUs. FACS defines 44 AUs and each AU intensity is defined on a nonlinear scale of five grades. There has been sig-nificant progress in the literature on the detection of AUs. However, the companion problem of estimating the AU strengths has not been much investigated. In this work we propose a novel AU intensity estimation scheme applied to 2D luminance and/or 3D surface geometry images. Our scheme is based on regression of selected image features. These features are either non-specific, that is, those inherited from the AU detection algorithm, or are specific in that they are selected for the sole purpose of intensity estimation. For thorough-ness, various types of local 3D shape indicators have been considered, such as mean curvature, Gaussian cur-vature, shape index and curvedness, as well as their fusion. The feature selection from the initial plethora of Gabor moments is instrumented via a regression that optimizes the AU intensity predictions. Our AU inten-sity estimator is person-independent and when tested on 25 AUs that appear singly or in various combina-tions, it performs significantly better than the state-of-the-art method which is based on the margins of SVMs designed for AU detection. When evaluated comparatively, one can see that the 2D and 3D modalities have relative merits per upper face and lower face AUs, respectively, and that there is an overall improve-ment if 2D and 3D intensity estimations are used in fusion.},
author = {Savran, Arman and Sankur, Bulent and {Taha Bilge}, M},
doi = {10.1016/j.imavis.2011.11.008},
file = {:home/luka/Dropbox/Mendeley/Savran, Sankur, Taha Bilge/2012/Savran, Sankur, Taha Bilge - 2012 - Regression-based intensity estimation of facial action units ☆.pdf:pdf},
journal = {IMAVIS},
keywords = {3D facial expression recognition,Action unit intensity estimation,AdaBoost.RT,Facial Action Coding System,Feature selection,SVM regression},
pages = {774--784},
title = {{Regression-based intensity estimation of facial action units ☆}},
volume = {30},
year = {2012}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
booktitle = {2014 IEEE Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.2014.220},
file = {:home/luka/Dropbox/Mendeley/Taigman et al/2014/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
isbn = {978-1-4799-5118-5},
keywords = {3D face modeling,Agriculture,DeepFace,Face,Face recognition,LFW dataset,Shape,Solid modeling,Three-dimensional displays,Training,alignment step,deep neural network,face recognition,face representation,face verification,human-level performance,image representation,labeled faces in the wild,neural nets,piecewise affine transformation,representation step},
month = {jun},
pages = {1701--1708},
publisher = {IEEE},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909616},
year = {2014}
}
@incollection{JoshuaM.Susskind123,
abstract = {This book provides an overview of state of the art research in Affective Computing. It presents new ideas, original results and practical experiences in this increasingly important research field. The book consists of 23 chapters categorized into four sections. Since one of the most important means of human communication is facial expression, the first section of this book (Chapters 1 to 7) presents a research on synthesis and recognition of facial expressions. Given that we not only use the face but also body movements to express ourselves, in the second section (Chapters 8 to 11) we present a research on perception and generation of emotional expressions by using full-body motions. The third section of the book (Chapters 12 to 16) presents computational models on emotion, as well as findings from neuroscience research. In the last section of the book (Chapters 17 to 22) we present applications related to affective computing.},
author = {{Joshua M. Susskind}, Geoffrey E. Hinton and Anderson, Javier R. Movellan and Adam K.},
booktitle = {Affect. Comput.},
chapter = {23},
doi = {10.5772/56897},
editor = {Or, Jimmy},
file = {:home/luka/Dropbox/Mendeley/Joshua M. Susskind, Anderson/2008/joshfacechapter.pdf:pdf},
isbn = {978-3-902613-23-3},
pages = {452},
publisher = {I-Tech Education and Publishing},
title = {{Generating Facial Expressions with Deep Belief Nets}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/joshfacechapter.pdf},
year = {2008}
}
@book{tfd,
author = {{J. M. Susskind, A. K. Anderson}, and G. E. Hinton},
publisher = {Department of Computer Science, University of Toronto, Toronto},
title = {{The Toronto Face Database}},
year = {2010}
}
@inproceedings{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
booktitle = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:home/luka/Dropbox/Mendeley/Girshick et al/2014/Girshick et al. - 2013 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
month = {nov},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@inproceedings{Valstar,
abstract = {Despite efforts towards evaluation standards in facial expression analysis (e.g. FERA 2011), there is a need for up-to-date standardised evaluation procedures, focusing in particular on current challenges in the field. One of the challenges that is actively being addressed is the automatic estimation of expression intensities. To continue to provide a standardisation platform and to help the field progress beyond its current limitations, the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015) will challenge participants to estimate FACS Action Unit (AU) intensity as well as AU occurrence on a common benchmark dataset with reliable manual annotations. Evaluation will be done using a clear and well-defined protocol. In this paper we present the second such challenge in automatic recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on Face and Gesture Recognition, May 2015, in Ljubljana, Slovenia. Three sub-challenges are defined: the detection of AU occurrence, the estimation of AU intensity for pre-segmented data, and fully automatic AU intensity estimation. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for the three sub-challenges.},
author = {Valstar, Michel F and Almaev, Timur and Girard, Jeffrey M and McKeown, Gary and Mehu, Marc and Yin, Lijun and Pantic, Maja and Cohn, Jeffrey F},
booktitle = {2015 11th IEEE Int. Conf. Work. Autom. Face Gesture Recognit.},
doi = {10.1109/FG.2015.7284874},
file = {:home/luka/Dropbox/Mendeley/Valstar et al/2015/Valstar et al. - 2015 - FERA 2015 - second Facial Expression Recognition and Analysis challenge.pdf:pdf},
isbn = {978-1-4799-6026-2},
keywords = {AU intensity,AU occurrence,Databases,Estimation,FACS action unit,FERA 2015,Face recognition,Feature extraction,Gold,Reliability,Training,emotion recognition,expression intensities estimation,face recognition,facial expression analysis,facial expression recognition and analysis challen,standardisation platform},
pages = {1--8},
title = {{FERA 2015 - second Facial Expression Recognition and Analysis challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7284874},
volume = {06},
year = {2015}
}
@article{googlepretrain,
abstract = {Whereas theoretical work suggests that deep architectures might be more efﬁcient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples.},
author = {Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal},
file = {:home/luka/Dropbox/Mendeley/Erhan et al/2009/Erhan et al. - 2009 - The difficulty of training deep architectures and the effect of unsupervised pre-training.pdf:pdf},
isbn = {15324435},
issn = {15324435},
journal = {Int. Conf. Artif. Intell. Stat.},
pages = {153--160},
title = {{The difficulty of training deep architectures and the effect of unsupervised pre-training}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS09{\_}ErhanMBBV.pdf},
volume = {5},
year = {2009}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/luka/Dropbox/Mendeley/Krizhevsky, Sutskever, Hinton/2012/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M Cm and Nasrabadi, Nm},
booktitle = {Pattern Recognit.},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {9780387310732},
issn = {10179909},
pages = {738},
pmid = {8943268},
publisher = {Springer},
title = {{Pattern recognition and machine learning}},
volume = {4},
year = {2006}
}
@article{Corneanu2016,
abstract = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
author = {Corneanu, Ciprian A and Oliu, Marc and Cohn, Jeffrey F and Escalera, Sergio},
doi = {10.1109/TPAMI.2016.2515606},
issn = {1939-3539},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {jan},
pmid = {26761193},
title = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-related Applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26761193},
year = {2016}
}
@book{Bishop1995,
abstract = {1. Statistical Pattern Recognition -- 2. Probability Density Estimation -- 3. Single-Layer Networks -- 4. The Multi-layer Perceptron -- 5. Radial Basis Functions -- 6. Error Functions -- 7. Parameter Optimization Algorithms -- 8. Pre-processing and Feature Extraction -- 9. Learning and Generalization -- 10. Bayesian Techniques.},
author = {Bishop, Christopher M.},
file = {:home/luka/Dropbox/Mendeley/Bishop/1995/Neural{\_}Networks{\_}for{\_}Pattern{\_}Recognition{\_}-{\_}Christopher{\_}Bishop.pdf:pdf},
isbn = {9780198538646},
pages = {482},
publisher = {Clarendon Press},
title = {{Neural networks for pattern recognition}},
year = {1995}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new ran- domized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:home/luka/Dropbox/Mendeley/Xu et al/2015/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in Convolution Network.pdf:pdf},
journal = {ICML Deep Learn. Work.},
pages = {1--5},
title = {{Empirical Evaluation of Rectified Activations in Convolution Network}},
year = {2015}
}
@misc{StephenMerity2016,
author = {{Stephen Merity}},
booktitle = {http://smerity.com/},
title = {{In deep learning, architecture engineering is the new feature engineering}},
url = {http://smerity.com/articles/2016/architectures{\_}are{\_}the{\_}new{\_}feature{\_}engineering.html},
urldate = {2016-09-01},
year = {2016}
}
@article{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/luka/Dropbox/Mendeley/Kingma, Ba/2014/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Lucey2010,
abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this pe-riod, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algo-rithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22{\%} and the number of subjects by 27{\%}. The target expres-sion for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a lin-ear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
author = {Lucey, Patrick and Cohn, Jeffrey F and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
file = {:home/luka/Dropbox/Mendeley/Lucey et al/2010/Lucey et al. - Unknown - The Extended Cohn-Kanade Dataset (CK) A complete dataset for action unit and emotion-specified expression.pdf:pdf},
isbn = {9781424470303},
journal = {IEEE Conf. Comput. Vis. Pattern Recognit. Work.},
number = {July},
pages = {94--101},
title = {{The extended cohn-kande dataset (CK+): A complete facial expression dataset for action unit and emotionspeciﬁed expression}},
year = {2010}
}
@article{autong,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:home/luka/Dropbox/Mendeley/Ng/2011/Ng - 2011 - Sparse autoencoder.pdf:pdf},
institution = {Stanford},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lect. notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{Ng2004,
abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
address = {New York, New York, USA},
author = {Ng, Andrew Y.},
doi = {10.1145/1015330.1015435},
file = {:home/luka/Dropbox/Mendeley/Ng/2004/Ng - 2004 - Feature selection, L1 vs. L2 regularization, and rotational invariance.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
journal = {Twenty-first Int. Conf. Mach. Learn. - ICML '04},
pages = {78},
pmid = {15040217},
publisher = {ACM Press},
title = {{Feature selection, L1 vs. L2 regularization, and rotational invariance}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015435},
year = {2004}
}

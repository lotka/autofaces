\chapter{Deep Learning Background} \label{chapter:deep}
  \section{Introduction}
    Deep learning algorithms have recently been applied to image classification
    and object detection tasks with lots of success. \cite{Girshick2014,Krizhevsky2012}
    Three main factors have been instrumental in making this happen \cite{Jaiswal2016}:
    \begin{itemize}
      \item Efficient methods to train deep artificial neural networks
      \item Improved parallel processing hardware such as GPU's
      \item An abundance of labelled data
    \end{itemize}
    This development has moved the focus of classification tasks away from feature engineering,
    which seeks to find general methods of quantifying and locating features in image data, to
    architecture design \cite{StephenMerity2016}, where the primary concern is the structure and size of the neural network that has to
    learn the desired task.

    The following section explains the key components that a deep learning algorithm requires,
    derivations of gradients for back propagation are left out as the focus of the project is to
    see how these components can interact and be built up and such derivations would add little,
    for a detailed analysis see \cite{Bishop1995}.
  \section{Model Components}
    \subsection{Artificial Neural Networks} \label{sec:anns}
      An artificial neuron is a function which takes in a vector of inputs, computes their
      weighted sum and applies a non-linearity as follows:
      \begin{equation}
        a(\mathbf{x}) = \sigma \left ( \sum_{i=1}^N w_ix_i + b \right )
      \end{equation}
      Here $\sigma$ is a scalar function, $w_i$ is a scalar weight and $b$ is its bias. $N$ is the size of the input and weight vector.
      These neurons can be combined into layered networks to construct artificial neural networks.
      The weights $w$ can then be rewritten as matrices $\mathbf{W}$ which define how
      activations from one layer are transferred to the next\footnote{Note: in this report when a vector
      is put into a scalar function it is assumed that it acts on each element of
      the vector. The only exception is with the softmax.}.
      \begin{equation}
        \mathbf{a}(\mathbf{x}) = \mathbf{\sigma} \left ( \mathbf{W}\mathbf{x} + \mathbf{b} \right ) \label{eq:softmax}
      \end{equation}

      Popular activation functions include:
      \begin{multicols}{2}


        \begin{equation}
          \text{The sigmoid:}\quad
          \sigma (x) = \frac{1}{1-e^{-x}} \label{eq:sigmoid}
        \end{equation}


        \begin{equation}
          \text{ReLU:}\quad
          \sigma(x) = \max(0,x)
        \end{equation}

        \begin{equation}
          \text{Leaky ReLU:}\quad
          \sigma(x) = \max(\alpha x,x)
        \end{equation}


        \begin{equation}
          \text{Hyperbolic tan:}\quad
          \sigma(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}
        \end{equation}


        \begin{equation}
          \text{The softmax:}\quad
          \sigma(\mathbf{x})_j = \frac{e^{x_j}}{\sum_i e^{x_i}}
        \end{equation}


      \end{multicols}

      For the leaky ReLU $\alpha$ is some small constant, the purpose is to allow gradients to carry on
      propagating if activations fall below zero. This approach has been proven to improve performance in some classification
      tasks \cite{Xu2015}.

      \begin{figure} \label{disfagraph}
        \center
        \includegraphics[width=.5\textwidth]{../graphs/actfuncs.pdf}
        \caption{Comparison of common activation functions for neural networks}
      \end{figure}



      An $N$ layer network can then be thought of as a function with a nested
      structure (these are also known as fully connected layers):
      \begin{equation} \label{eq:ynn}
        \mathbf{y} = \mathbf{a}_{N}(\mathbf{a}_{l-1}(...\mathbf{a}_1(\mathbf{x})...))
      \end{equation}
      Now assuming there exists a set of $\tilde{\mathbf{x}}$ and $\tilde{\mathbf{y}}$ which make
      up a training data set, these could be sets of images and labels, then a cost function
      to optimise can be defined:
      \begin{equation}
        J(\tilde{\mathbf{x}},\tilde{\mathbf{y}},\mathbf{y}) = \frac{1}{N}\left |\mathbf{y}(\tilde{\mathbf{x}})-\tilde{\mathbf{y}}\right | ^2
      \end{equation}
      this is called the mean squared error, here $\tilde{\mathbf{x}}$ and $\tilde{\mathbf{y}}$ could
      be the images and labels of a dataset respecitvely and $\mathbf{y}$ represents the network we are evaluating (equation \ref{eq:ynn}). Another popular cost function is
      the cross entropy:
      \begin{equation}
        J(\tilde{\mathbf{x}},\tilde{\mathbf{y}},\mathbf{y}) = -\frac{1}{N}\tilde{\mathbf{y}}\cdot\log(\mathbf{y}(\tilde{\mathbf{x}}))
      \end{equation}

      This has the property of giving high costs when a postive exampe is incorrectly classified as negative: $\lim _{y\rightarrow0}\frac{1}{N}1\cdot \log(y) = \infty$
      and not adjusting the weights otherwise.

    \subsection{Training}
        This report will often refer to the process of training, the objective of training is to find
        a $\tilde{\mathbf{y}}^* \in \mathbb{R}^{N\times M...}$ that satisifies the following definition:

        \begin{equation}
          J(\tilde{\mathbf{x}},\tilde{\mathbf{y}},\mathbf{y}^*) \leq J(\tilde{\mathbf{x}},\tilde{\mathbf{y}},\mathbf{y}) \quad \forall \mathbf{y} \in \mathbf{Y}
        \end{equation}

        Where $\mathbf{Y}$ is defined as the set of all possible $\mathbf{y}$ functions which are constructed with a series $\mathbf{W}$ and $\mathbf{b}$ matrixes as defined used in the previous section.
        As $J$ cannot be known fully and is unlikely to be convex \footnote{Which would garuentee a global minimum, for example a parabola $y(x)=x^2$ is convex and has a minimum at $y(0)$}
        , this is impracticle to achieve in practice.
        Training is therefore the process of finding a local minimum by taking a path through $J$ by varying
        the function $ \mathbf{y} $, the different stratergies by which $J$ is explored are called
        training algorithms or optimizers.

        This is actualisied by taking the derivative, with respect to each weight variable
        in the network, $i$ and updating those variables in order to reduce $J$:

        \begin{equation}
          w_i \leftarrow w_i - \eta \frac{\partial J }{\partial w_{i}}(\tilde{\mathbf{x}},\tilde{\mathbf{y}},\mathbf{y})
        \end{equation}

        This occurs for every $i$ where $w_i$ is some trainable variable in $\mathbf{y}$.
        This is the simplest way to achieve training or gradient descent, this is refered to as backpropagation
        because computing the derivative requires the repeated use of the chain rule.

        If $\tilde{x}$ is a set of examples then this is batch gradient descent, if the examples
        are randomly selected from the dataset then it is stocastic gradient descent.

        \todo{AAAAAADAMMMMMmmmm}



        %
        % The derivative of these cost functions with respect to each weight and bias variable
        % in the network is then calculated.
        % The simplest way is to do this is per training example, however stochastic gradient
        % descent\cite{Amari1993} has emerged as a superior method. Simply put it computes the average gradient with respect
        % to many randomly drawn samples, it has the advantage of following a smoother path
        % towards the local minimum. Another training algorithm which builds on this is called is Adam \cite{adam}.
        % http://sebastianruder.com/optimizing-gradient-descent/index.html#adam
        % \url{https://www.quora.com/Can-you-explain-basic-intuition-behind-ADAM-a-method-for-stochastic-optimization}
    \subsection{Convolutional Layers}
      A convolutional layer is a generalisation of simple fully connected layers described
      above. It consists of a set of $K$ filters of size $m\times m$, which are applied to the input to produce
      a set of $K$ outputs. The filters are applied with a 2D convolution.


      To describe them, firstly assume that any vector described in the previous section
      can also be rewritten as a matrix, i.e $\mathbf{x} \in \mathbb{R}^{n}
      \rightarrow \mathbf{x} \in \mathbb{R}^{N \times M} \quad NM=n$, in practice this is made
      easy by using numbers which factorise well.

      Then the following equation describes the output of a convolutional layer:
      \begin{equation} \label{CNN}
        \mathbf{a}(\mathbf{x})_{ijk} = \sigma \left ( \sum_{a=0}^{m-1}\sum_{b=0}^{m-1}(w_{abk}x_{(i+a)(j+b)k}) + b_k \right )
      \end{equation}
      Here $i,j$  denote row, column indices for the matrix (image) $\mathbf{x}$, $k$ is the filter index, $w_{abk}$
      gives the filter element and $b_k$ is the bias for that filter. This is done for all $K$ filters.

      One issue is how to deal with indices which are out of bounds, SAME padding can be used which sets out of bounds
      elements to zero and so preserves the image size or VALID can be used, this keeps the filter within the bounds of the
      image and hence the output is of a smaller dimension. Lastly a stride greater than 1 can be incorporated into equation
      \ref{CNN} meaning that $\mathbf{a}$ is only computed for a fraction of indices $i,j$.
      % \begin{equation}
      % 	C : \mathbb{R}^{n\times m \times p \times l} \rightarrow \mathbb{R}^{N\times M \times l \times K}
      % \end{equation}
      %con%
    \subsection{Max Pooling Layers}
      A max pooling layer simply splits its input into a set of sections and extracts
      the highest value from each. It is a simple but effective method to down sample
      an image, it helps to keep the computational overhead of these algorithms
      down. However a recent trend \cite{Springenberg2015} has emerged where max pooling
      layers are not used, instead a convolutional layer with a large stride accomplishes
      the same amount of down-sampling.
    \subsection{Autoencoders}
      An autoencoder is at its bare minimum a artificial neural network which tries
      to reproduce its input as accurately as possible. So the cost function for the mean squared error becomes:

      \begin{equation} \label{eq:autoencoder_cost}
        J(\tilde{\mathbf{x}},\tilde{\mathbf{y}}) = \frac{1}{N}\left |\mathbf{y}(\tilde{\mathbf{x}})-\tilde{\mathbf{x}}\right | ^2
      \end{equation}

      Constraints are
      placed on the network so that it has to learn to compress the input, the following
      are popular constraints that may be used:
      \begin{itemize}
        \item Few neurons in the hidden layers
        \item Sparsity: the average activation of the neurons can be kept under a
        threshold, typically a small value close to zero \cite{autong}
        \item Noise may be added to the input, this makes the network more likely
        to learn general features.
      \end{itemize}
      Lastly there are Variational Autoencoders which combine ideas from Bayesian inference
      to create networks which can not only reconstruct their input but also act as a
      distribution which can be sampled from, allowing for the generation of new samples \cite{Kingma2013}.

    \subsection{Joint vs Stacked Autoencoder training}
      A stacked autoencoder is typically used for pre-training a network for a classification task.
      Instead of training the whole structure at once, each layer is trained as the last hidden
      layer of some temporary autoencoder. It has been shown that this can improve classification performance \cite{stacks}.
    \subsection{Convolutional Autoencoder}
      A convolutional autoencoder works in the same way as a standard autoencoder, however
      undoing the max pooling presents a problem, as a max pooling layer throws away
      information, its inverse will never be exact. The following strategies have been
      used with success:
      \begin{itemize}
        \item Replacing each entry with an $n \times n$ matrix filled with the original
        entry.
        \item Replacing each entry with an  $n\times n$  matrix with
        the original entry in the upper left and the other squares set to 0. \cite{Dosovitskiy2015}
      \end{itemize}

      Other than that all other elements have very straightforward inverses and hence
      a convolutional autoencoder can be constructed.
    \subsection{Local Response Normalisation} \label{sec:lrn}
      This section reproduces the local response normalisation proceedure described in \cite{Krizhevsky2012}
      and implemented as the function \texttt{tf.nn.lrn} in tensorflow.

      It was found to aid generalisation with the ImageNet dataset. The following quote from the paper \cite{Krizhevsky2012}
      describes the motivation for this approach:

      \begin{displayquote}
        This sort of response normalization implements a form of lateral inhibition
        inspired by the type found in real neurons, creating competition for big activities amongst neuron
        outputs computed using different kernels.
      \end{displayquote}

      This is done by normalising over the feature maps which are computed by the different convolutional kernels (or filters).
      The following equation shows the proceedure applied to the output of a convolutional layer:

      \begin{equation} \label{eq:lrn}
        \mathbf{a}(\mathbf{x})_{ijk}
        = \mathbf{a}(\mathbf{x})_{ijk}/\left (\rho + \sum^{\min(N-1,i+n/2)}_{m=\max(0,i-n/2)}\mathbf{b}(\mathbf{x})_{ijm} \right )^\beta
      \end{equation}

      the parameter $n$ signifies how many adjacent feature maps to sum over, $N$ is the total number of feature maps.
      Parameters $\rho,n,\alpha$ and $\beta$ are hyperparameters, the values used in \cite{Krizhevsky2012} are
      $k=2,n=5,\alpha=10^{-4}$ and $\beta=0.75$.

  \section{Overfitting and Regularisation}
    A machine learning model will always perform better on the set used
    to train it, 
    % http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization
    \subsection{Denoising}
      {\color{red} TODO}
    \subsection{Regularisers}
      {\color{red} TODO}
    \subsection{Dropout Layers} \label{sec:dropout}
      A layer with dropout applied to it randomly turns neurons off, making it more
      difficult for the network to overfit data. There is a dropout probability
      which determines how often neurons are disabled, it is typically under 20\%
      {\color{red} TODO: Add some references}
    \subsection{Early Stopping}
      {\color{red} TODO}
  \section{Model Evaluation} \label{sec:eval}
    With any kind of classification model it is crucial to be able to evaluate its
    performance in a standard and reproducible way. For the problem of classifying
    AUs the simplest case is to split the problem into a set of $n_C$ decision problems
    where $n_C$ is the number of distinct classes. Now the problem is reduced to
    evaluating the performance of a binary classifier.

    \subsection{Recall, Precision and F1}
      Naively the first metric one might use for measuring the performance of a classifier
      might be the accuracy, this is good for datasets which are evenly balanced however
      the datasets of interest in this report typically have many more negative labels than postive labels.
      A classifier that simply labels all examples negative might then get a very high accuracy. This motivates
      defining a matrix called the confusion matrix whose elements express accuracy from various angles.
      For a binary problem the confusion matrix is defined as:
      \begin{equation}
        C =
        \begin{pmatrix}
          TP & FN\\
          FP & TN
        \end{pmatrix}
      \end{equation}
      Where $TP$ is the number of true positives, $FN$ is the number of false negatives
      , $FP$ is the number of true positives and $TN$ is the number of true negatives.
      In a perfect classification, this matrix would have $FP=FN=0$, however this is
      difficult to achieve
      and we can define quantities to measure how close to this ideal we are. Note we
      only define the binary case here, the more general confusion matrix for multiple
      classes can easily be defined but is not relevant here.
      \begin{equation}
        \text{Recall} = \frac{TP}{TP+FN}
      \end{equation}
      \begin{equation}
        \text{Precision} = \frac{TP}{TP+FP}
      \end{equation}
      \begin{equation}
        \text{F1} = 2 \cdot \frac{\text{Recall} \cdot \text{Precision}}{\text{Recall} + \text{Precision}}
      \end{equation}

      Hence recall is decreased by false negatives, i.e not being able to recall the class
      when presented with it. Precision is decreased by false positives i.e stating the class
      is present when it is not. Both measures describe a different aspect of the accruacy. This
      motivates defining the F1 score which is the harmonic mean between the precision and recall
      (the harmonic mean is used to ensure that case where $R=1$ and $P=0$ R,P being Recall and Precision, gives
      a very low score  as achieving $R=1$ is trivial.) The F1 is the quantity this report will seek to maximise and use to compare
      results with the literature.
    \subsection{Receiver Operating Characteristics}
      Another useful measure is the area under the Receiver operating characteristic (ROC) curve.
      As neural networks output a number between 0 and 1, a threshold must be chosen to signify
      when the network declares a class present. By varying this threshold a series of true positive and false positive rate points
      can be generated, this is the ROC curve. The area under this is ideally 1 and in the worst case 0.5 (TP = FP), hence this is
      another measure of classification accuracy that is invariant to the chosen threshold.

      \begin{table}[]
        \centering \caption{A simple qualitative illustration of what a ROC score means for a classifier.
        Note: It is typically only in strange cases that $ROC< 0.5$ but is possible.} \label{my-label}
        \begin{tabular}{rl}
          \hline
          Range & Classifier quailty \\ \hline
          $0 \leq ROC \leq 0.6$   & Fail               \\
          $0.6 \leq ROC    < 0.7$   & Poor               \\
          $0.7 \leq ROC    < 0.8$   & Fair               \\
          $0.8 \leq ROC    < 0.9$   & Good               \\
          $0.9 \leq ROC \leq 1.0$   & Great              \\ \hline
        \end{tabular}
    \end{table}

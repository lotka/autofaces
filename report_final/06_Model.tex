\chapter{Model} \label{sec:model}
  % This chapter describes the path taken in constructing the autoencoding classifier (AEC) network, detailing
  % the experiments which were performed to create the final configuration.
  % The most important metric used to judge a network in the chapter is this Average ROC score of the classifier
  % when applied to the un-seen test set, the secondary metric is the error
  % in the reconstructed image outputted from the autoencoder. The overall aim is to
  % find a configuration where the presence of the autoencoder improves
  % the performance of the classifier and secondarily where improving the secondary metric (the autoencoder) improves
  % the primary metric (the classifier).

  \section{Experimental Set Up}
    In order to develop the network for modelling the DISFA dataset a set of common
    parameters for experimentation are useful to define, these are shown in table \ref{tab:common}

    \begin{table}[!h]
      \centering { \footnotesize
      \begin{tabular}{ll}
        \hline
        \textbf{Parameter}                       & \textbf{Value}                \\ \hline
        Optimizer                                & Adam                          \\
        Learning Rate                            & 0.001                         \\
        L2 Regularisation Coefficient            & 0.0                           \\
        Hidden layer activation functions         & Leaky ReLu                          \\
        Image downscaling                        & 0.4                           \\
        AU's present                             & 1,2,4,5,6,9,12,15,17,20,25,26 \\
        Iterations:                              & 1000                          \\
        Early model save percent                 & 50\%                          \\
        Weight tensor initial standard deviation & 0.001                         \\
        Bias tensor initial value                & 0.01                          \\
        Train batch size                         & 100                           \\
        Validation batch size                    & 500                           \\
        Penultimate fully connected layer size   & 3000                          \\
        Autoencoder cost function                & Mean Squared Error            \\
        Classifier cost function                 & Cross Entropy                 \\
        \hline
      \end{tabular}
      \caption{Common parameters for most experiments in the section. These parameters should be
      assumed in proceeding sections unless otherwise stated.} \label{tab:common} }
    \end{table}

    The cost functions were chosen as they are typical of their use cases, the cross entropy is more suited
    for classification problems as it penalises heavily incorrect answers and the meaned squared

    For all of the experiments the data was partitioned in a 50/50 split as shown in table \ref{sec:splitting}:
    \begin{table}[h!]
      \centering { \footnotesize
      \begin{tabular}{|l|l|}
      \hline
      Set & Subjects   \\
      \hline
       Train          & 2,4,6,8,10,12,16,18,23,25,27,29,31      \\
      \hline
      Validation      & 1,3,5,7,9,11,13,17,21,24,26,28,30,32 minus test set     \\
      \hline
      Test           & 500 chosen randomly taken from validation set      \\
     \hline
      \end{tabular}
      \caption{The split of the DISFA dataset used in the experiments.}
      \label{sec:splitting} }
    \end{table}

    For the labels AU intensities 1-5 counted as a postive example and AU intesnity 0 counted as a negative one.
    No examples were removed from the training set.


  \section{Joint classification}

    Typical deep neural networks are used to classify
    one image into only one category. The case with FAU detection is different, we
    would like to be able to classify the categories jointly,  putting one frame into more than
    one AU category. Ideally the network would output a confidence score between 0 and 1
    to signify if an AU is present and we would calculate some optimum threshold value
    (ideally this would be 0.5).

    The following list details three possible solutions to the joint classification problem:

    \begin{itemize} \label{sec:binsoft}
      \item {\bf Softmax Layer} - This is a traditional, fully connected layer with
      a softmax activation function (see equation \ref{eq:softmax}).
      The issue with this is that it provides a probability distribution over AUs
      but the required quantity is a probability distribution for each AU.
      \item {\bf Sigmoid Layer} - This again is like the previous solution but instead each neuron gives a confidence
      score between 0 and 1 for each AU with a sigmoid function (see equation \ref{eq:sigmoid}).
      The issue with this however is that sigmoid
      functions have vanishing gradients at large input values
      hence training may become difficult.
      \item {\bf Binary Softmax Layers} - Here there is a two neuron softmax layer
      for each AU, this doubles the amount of weights
      but gives a probability over the presence and
      absence of each AU which is ideal. In the implementation
      the negative neuron of each binary layer is only used for training purposes, and it is
      discarded while evaluating the classification performance.
    \end{itemize}




  \section{Network Structures}

    \begin{table}[h!]
    \centering
    {\footnotesize
    \begin{tabular}{|lllllllll|}
    \hline
    \multicolumn{1}{|l|}{Element} & Type     & \multicolumn{1}{l|}{Dimensions}                     & Type     & \multicolumn{1}{l|}{Dimensions}                      \\ \hline
    \multicolumn{1}{|l|}{x}       &          & \multicolumn{1}{l|}{$47\times47\times1$}            &          & \multicolumn{1}{l|}{}                                \\ \hline
    \multicolumn{1}{|l|}{$L_1$}   & fc       & \multicolumn{1}{l|}{$2209\times14$}              & Binary Softmax & \multicolumn{1}{l|}{$3000\times2\times12$}        \\
    \multicolumn{1}{|l|}{$y_1$}   & dropout  & \multicolumn{1}{l|}{$14$}                         &          & \multicolumn{1}{l|}{$24$}                              \\ \hline
    \multicolumn{1}{|l|}{$L_2$}   & fc       & \multicolumn{1}{l|}{$14\times2209$}              &          & \multicolumn{1}{l|}{}                                   \\
    \multicolumn{1}{|l|}{$y_2$}   &          & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{}                                \\ \hline
    \multicolumn{1}{|l|}{$L_3$}   & reshape & \multicolumn{1}{l|}{}                    &          & \multicolumn{1}{l|}{}                                \\
    \multicolumn{1}{|l|}{$y_3$}   &          & \multicolumn{1}{l|}{$47\times47\times 1$}          &          & \multicolumn{1}{l|}{}                                \\ \hline
    \end{tabular}

    \caption{} \label{net:simple1}

    }
    \end{table}

    \begin{figure}[h!]
     \centering
     \includegraphics[width=\textwidth]{illustrations/aec_network.pdf}
     \captionof{figure}{A diagram of network 2 used in this section.}
    \end{figure}

    Now that some basic structures have been tested we proceed to convolutional networks,
    inspired by the literature (see table \ref{tab:compnet}) three networks are defined.
    Network 2 is shown in table \ref{tab:net2}, Network 3
    adds another convolutional layer of size $5\times 5 \times 64 \times 64$ to this and
    Network 4 adds a $4\times 4 \times 64 \times 128$ layer
    on top of Network 3. Regarding the decoder section the inverse operations are
    performed, see appendix \label{appendix1} for full details
    for networks 2,3 and 4.

    % network.append(cnn_layer(ll(network), [5, 5, 64, 64], 'VALID', 'Convolution_2', config, act=act))
    % network.append(cnn_layer(ll(network), [4, 4, 64, 128], 'VALID', 'Convolution_3', config, act=act))

    \begin{table}[h!] \caption*{\textbf{Network 2}}
    \centering
    {\footnotesize
    \begin{tabular}{|lllllllll|}
    \hline
    \multicolumn{1}{|l|}{Element} & Type     & \multicolumn{1}{l|}{Dimensions}                     & Type     & \multicolumn{1}{l|}{Dimensions}  \\ \hline
    \multicolumn{1}{|l|}{x}       &          & \multicolumn{1}{l|}{$47\times47\times1$}            &          & \multicolumn{1}{l|}{}            \\ \hline
    \multicolumn{1}{|l|}{$L_1$}   & conv 1   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}            \\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$43\times43\times64$}           &          & \multicolumn{1}{l|}{}            \\ \hline
    \multicolumn{1}{|l|}{$L_2$}   & max pool & \multicolumn{1}{l|}{$2\times 2$}                    &          & \multicolumn{1}{l|}{}            \\
    \multicolumn{1}{|l|}{$y_2$}   &          & \multicolumn{1}{l|}{$22\times22\times 64$}          &          & \multicolumn{1}{l|}{}            \\ \hline
    \multicolumn{1}{|l|}{$L_3$*}   & fc       & \multicolumn{1}{l|}{$30976\times3000$}              & Binary
                                                                                                      Softmax & \multicolumn{1}{l|}{$3000\times2\times12$}        \\
    \multicolumn{1}{|l|}{$y_3$}   & dropout  & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{$24$}        \\ \hline
    \multicolumn{1}{|l|}{$L_4$}   & fc       & \multicolumn{1}{l|}{$3000\times30976$}              &          & \multicolumn{1}{l|}{}            \\
    \multicolumn{1}{|l|}{$y_4$}   &          & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{}            \\ \hline
    \multicolumn{1}{|l|}{$L_5$}   & resize\& reshape & \multicolumn{1}{l|}{$2$}                    &          & \multicolumn{1}{l|}{}            \\
    \multicolumn{1}{|l|}{$y_5$}   &            & \multicolumn{1}{l|}{$43\times43\times 64$}          &          & \multicolumn{1}{l|}{}            \\ \hline
    \multicolumn{1}{|l|}{$L_6$}   & deconv 1   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}  &          & \multicolumn{1}{l|}{}            \\
    \multicolumn{1}{|l|}{$y_6$}   &            & \multicolumn{1}{l|}{$47\times47\times1$}            &          & \multicolumn{1}{l|}{}             \\ \hline
    \end{tabular}

    \caption{A network used many times in the report, for further details see appendix \ref{appendix1} \label{tab:net2}
    \newline *Bottleneck layer}
    }
    \end{table}

    \begin{table}[h!]
    \centering
    \caption*{{\bf \large Network 3}}
    {\footnotesize
    \begin{tabular}{|lllllllll|}
    \hline
    \multicolumn{1}{|l|}{Element} & Type     & \multicolumn{1}{l|}{Dimensions}                     & Type     & \multicolumn{1}{l|}{Dimensions}  \\ \hline
    \multicolumn{1}{|l|}{x}       &          & \multicolumn{1}{l|}{$47\times47\times1$}            &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_1$}   & conv 1   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$43\times43\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_2$}   & max pool & \multicolumn{1}{l|}{$2\times 2$}                    &          & \multicolumn{1}{l|}{}        \\
    \multicolumn{1}{|l|}{$y_2$}   &          & \multicolumn{1}{l|}{$22\times22\times 64$}          &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_1$}   & conv 2   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$18\times18\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_3$}   & fc       & \multicolumn{1}{l|}{$20736\times3000$}              & Binary Softmax & \multicolumn{1}{l|}{$3000\times2\times12$}        \\
    \multicolumn{1}{|l|}{$y_3$}   & dropout  & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{$24$}        \\ \hline

    \multicolumn{1}{|l|}{$L_4$}   & fc       & \multicolumn{1}{l|}{$3000\times20736$}              &          & \multicolumn{1}{l|}{}        \\
    \multicolumn{1}{|l|}{$y_4$}   &          & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_5$}   & resize\& reshape & \multicolumn{1}{l|}{$2$}                    &          & \multicolumn{1}{l|}{}        \\
    \multicolumn{1}{|l|}{$y_5$}   &          & \multicolumn{1}{l|}{$18\times18\times 64$}          &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_6$}   & deconv 2   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_6$}   &          & \multicolumn{1}{l|}{$22\times22\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline


    \multicolumn{1}{|l|}{$L_7$}   & deconv 1   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_7$}   &          & \multicolumn{1}{l|}{$47\times47\times1$}           &          & \multicolumn{1}{l|}{}        \\ \hline
    \end{tabular}
    \caption{} \label{net:2}
    }
    \end{table}

    \begin{table}[h!]
    \centering
    \caption*{{\bf \large Network 4}}
    {\footnotesize
    \begin{tabular}{|lllllllll|}
    \hline
    \multicolumn{1}{|l|}{Element} & Type     & \multicolumn{1}{l|}{Dimensions}                     & Type     & \multicolumn{1}{l|}{Dimensions}  \\ \hline
    \multicolumn{1}{|l|}{x}       &          & \multicolumn{1}{l|}{$47\times47\times1$}            &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_1$}   & conv 1   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$43\times43\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_2$}   & max pool & \multicolumn{1}{l|}{$2\times 2$}                    &          & \multicolumn{1}{l|}{}        \\
    \multicolumn{1}{|l|}{$y_2$}   &          & \multicolumn{1}{l|}{$22\times22\times 64$}          &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_1$}   & conv 2   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$18\times18\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_1$}   & conv 3   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$15\times15\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_3$}   & fc       & \multicolumn{1}{l|}{$14400\times3000$}              & Binary Softmax & \multicolumn{1}{l|}{$3000\times2\times12$}        \\
    \multicolumn{1}{|l|}{$y_3$}   & dropout  & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{$24$}        \\ \hline

    \multicolumn{1}{|l|}{$L_4$}   & fc       & \multicolumn{1}{l|}{$3000\times 14400$}              &          & \multicolumn{1}{l|}{}        \\
    \multicolumn{1}{|l|}{$y_4$}   &          & \multicolumn{1}{l|}{$3000$}                         &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_5$}   & resize\& reshape & \multicolumn{1}{l|}{$2$}                    &          & \multicolumn{1}{l|}{}        \\
    \multicolumn{1}{|l|}{$y_5$}   &          & \multicolumn{1}{l|}{$15\times15\times 64$}          &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_1$}   & deconv 3   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_1$}   &          & \multicolumn{1}{l|}{$18\times18\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline

    \multicolumn{1}{|l|}{$L_6$}   & deconv 2   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_6$}   &          & \multicolumn{1}{l|}{$22\times22\times64$}           &          & \multicolumn{1}{l|}{}        \\ \hline


    \multicolumn{1}{|l|}{$L_7$}   & deconv 1   & \multicolumn{1}{l|}{$5\times 5\times1\times 64$}    &          & \multicolumn{1}{l|}{}\\
    \multicolumn{1}{|l|}{$y_7$}   &          & \multicolumn{1}{l|}{$47\times47\times1$}           &          & \multicolumn{1}{l|}{}        \\ \hline
    \end{tabular}
    \caption{} \label{net:2}
    }
    \end{table}
  \section{Sharing Weights}
    ONLY SHARE THE WEIGHTS BUT SHARE THEM ALL, THE BIASES NAH
    In order to reduce the number of parameters in an autoencoder, to avoid over fitting
    and the probability of learning the identity function, it is often helpful to share weights
    between the encoder and decoder sections.
  \section{Local Contrast Normalisation}
  \section{Dropout}
  \section{L2 Regularisation}
    L2 Regularisation incurs a penalty to using weights which have high values. This was implemented by adding L2
    loss terms for all weight variables. The new cost function becomes:

    \begin{equation} \label{eq:l2_cost_model}
        J(\tilde{\mathbf{x}},\tilde{\mathbf{y}}) = -\frac{1-\alpha(t)}{2FN}\tilde{\mathbf{y}}\cdot\log(\mathbf{y}(\tilde{\mathbf{x}}))
        + \frac{\alpha(t)}{N}\left |\mathbf{y}(\tilde{\mathbf{x}}) \odot \mathbf{M}-\tilde{\mathbf{x}}\right | ^2
        + \beta \sum_i^{\text{weights}}||\mathbf{W}_i||^2
    \end{equation}

    Where $\beta$ is some positive balancing factor and $\mathbf{W}$ signifies
    a weight tensor from any type of layer in the network.



  \section{Autoencoder classifier balancing}
    \label{sec:autoalpha}
    A key structure that is to be investigated in this report is a network with two objective functions:
    autoencoder and classifier. This is achieved by having a bottleneck layer where the branching occurs.
    The autoencoder has symmetry along this bottleneck, while the classifier consists of one further layer
    (the binary softmax classifier from the previous section). The cost function for the whole network is as follows:

    \begin{equation}
      J(\tilde{\mathbf{x}},\tilde{\mathbf{y}}) = -\frac{1-\alpha(t)}{2FN_B}\tilde{\mathbf{y}}\cdot\log(\mathbf{y}(\tilde{\mathbf{x}}))
      + \frac{\alpha(t)}{N_BN_P}\left |\mathbf{y}(\tilde{\mathbf{x}}) \odot \mathbf{M}-\tilde{\mathbf{x}}\right | ^2
    \end{equation}

    Where $\mathbf{M}$ is a mask described in section \ref{sec:mask}. Here F is the number of AU's, N is the size of the batch and $\alpha(t)$ is a
    function which balances the two costs. $\alpha(t)$ might be constant $\left ( \alpha_{\text{constant}}(t)=\frac{1}{2} \right )$ or
    some sort of polynomial which stays in the range $[0,1]$. $N_B$ and $N_P$ are the batch size and number of pixels respectively.

    For much of the initial investigations we employ:
    \begin{equation}
      \alpha_{\text{step}}(t,p,T) =
      \begin{cases}
        1           & \text{if}\ t<pT \\
        0           & \text{otherwise}
      \end{cases}
    \end{equation}

    Where $t$ indexes iterations, $T$ is the total number of iterations and p is the
    percentage of iterations that should be in the first region of the piecewise function
    where $\alpha=1$.
    This nicely expresses what is typically meant by pre-training in the literature, it trains
    the autoencoder up until iteration $pT$ (normally $p=\frac{1}{2}$ or $p=\frac{2}{3}$) and then the classifier for the remainder of the time.
    This acts as a good base case to test both sides of the network, later more interesting
    functions will be explored such as the sigmoid step:

    \begin{equation}
      \alpha_{\text{sigmoid}}(t,T,p,\tau) = \frac{1}{1 + \exp(1 + \tau (t/T - p))}
    \end{equation}

    Or a polynomial function:

    \begin{equation}
      \alpha_{\text{poly}}(t,T,n) = 1 - \left ( \frac{t}{T} \right )^n
    \end{equation}

    Or a periodic step function:

    \begin{equation}
      \alpha_{\text{alternate}}(t,T,p) =
      \begin{cases}
        1           & \text{if } t < |T/p| \text{ and } p > 0\\
        0           & \text{if } t < |T/p| \text{ and } p \leq 0\\
        \alpha_{\text{alternate}}(t-|T/p|,T,-p)           & \text{otherwise} \\
      \end{cases}
    \end{equation}

    Examples of such functions are plotted in figure \ref{fig:alpha_functions}.

    \begin{figure}[!h]
      \centering
      \includegraphics[width =\hsize]{figures/alpha.pdf}
      \caption{Examples of alpha transfer functions described in section \ref{sec:autoalpha}.
      For the polynomial case higher values of $n$ decrease the amount of training
      the classifier receives. While in the case of the step p dictates the training
      that each network receives, the sigmoid can be seen as a smoothed out step function
      with the two being equivalent as $ \tau \rightarrow \infty$. Similarly as $n \rightarrow \infty$
      the polynomial function turns into a constant function with $c=1$ except at the final
      iteration.}
      \label{fig:alpha_functions}
    \end{figure}

  \section{Qualitative Results}

\documentclass[11pt,twoside]{report}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Combining Autoencoding and Classifying Neural Networks for Joint Facial Action Unit Recognition}
\newcommand{\reportauthor}{Luka Milic}
\newcommand{\supervisor}{Sebastian Kaltzwang and Maja Pantic}
\newcommand{\degreetype}{Computing Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% load some definitions and default packages
\input{includes/includes}

% load some macros
\input{includes/notation}

\date{September 2016}
\begin{document}

% load title page
\input{includes/titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  % Datasets which contain labels of Facial Action Units (FACS) typically have
  % many unlabelled frames representing of neutral expressions. Training neural networks
  % on such datasets hence presents a challenge, as they typically require many labelled
  % examples. With this motivation a structure with both an autoencoding and classifer branch
  % is proposed and is extensively evaluated with a multitude of image preprocessing techniques.
  % The result is a slight improvement in the classification performance over the classifier only
  % version, furthermore the effect of dropout, L2 Regulurastion, denoising and local contrast
  % normalisation on the interaction between autoencoder and classifier are explored.
  % This is

  Autoencoders and classifiers can be constructed with neural networks and their combined training is of interest for improving classification results particuarly in sparsely labelled datasets. Datasets which have Facial Action units are in such a category and the DISFA dataset is used as a benchmark. The construction of a convolutional neural network with both structures is explored while utilising techniques such as dropout, local contrast normalisaiton and denoising autoencoders. Various transfer functions which govern the amount of training priority that the autoencoder and classifier branches recieve during training are tested and some improvemets in classification performance are observed.
\end{abstract}

% \cleardoublepage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Acknowledgements}
% Thanks to Sebastian Kaltzwang for

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

\input{01_Introduction}
\input{02_Background}
\input{03_DeepLearning}
\input{04_Implementation}
\input{05_Preprocess}
\input{06_Model}
\input{07_Discussion}
\input{08_Appendix}

%% bibliography
\bibliographystyle{unsrt}
\bibliography{bib/autofaces}
\end{document}

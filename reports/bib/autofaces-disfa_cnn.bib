Automatically generated by Mendeley Desktop 1.15.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Ghosh2015,
author = {Ghosh, Sayan and Laksana, Eugene and Scherer, Stefan and Morency, Louis-Philippe},
booktitle = {2015 Int. Conf. Affect. Comput. Intell. Interact.},
doi = {10.1109/ACII.2015.7344632},
file = {:vol/lm1015-tmp/mendeley/Ghosh et al/2015/Ghosh et al. - 2015 - A multi-label convolutional neural network approach to cross-domain action unit detection.pdf:pdf},
isbn = {978-1-4799-9953-8},
keywords = {AU detection,Action Units,BP4D AU dataset,CK+ AU dataset,Convolutional Neural Networks,Cross-dataset transfer,DISFA AU dataset,Face recognition,Feature extraction,Gold,Neural networks,Testing,Training,Videos,affective computing,classification task,cross-domain action unit detection,face recognition,facial images,feature extractors,image classification,image representation,input image,learning (artificial intelligence),multilabel convolutional neural network approach,neural nets,shared representation learning,testing condition,training condition,visual databases},
language = {English},
month = {sep},
pages = {609--615},
publisher = {IEEE},
title = {{A multi-label convolutional neural network approach to cross-domain action unit detection}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7344632},
year = {2015}
}
@article{Osendorfer2014,
abstract = {We present a computationally efficient architecture for image super-resolution that achieves state-of-the-art results on imageswith large spatial extend. Apart from utilizing Convolutional Neural Networks, our approach leverages recent advances in fast approximate inference for sparse coding.We empirically show that upsampling methods work much better on latent representations than in the original spatial domain. Our experiments indicate that the proposed architecture can serve as a basis for additional future improvements in image super-resolution.},
author = {Osendorfer, Christian and Soyer, Hubert and van der Smagt, Patrick},
doi = {10.1007/978-3-319-12643-2{\_}31},
file = {:vol/lm1015-tmp/mendeley/Osendorfer, Soyer, van der Smagt/2014/Osendorfer, Soyer, van der Smagt - 2014 - Image Super-Resolution with Fast Approximate Convolutional Sparse Coding.pdf:pdf},
isbn = {9783319126425},
issn = {16113349},
keywords = {convolutional neural,image processing,sparse coding},
pages = {250--257},
publisher = {Springer International Publishing},
title = {{Image Super-Resolution with Fast Approximate Convolutional Sparse Coding}},
url = {http://link.springer.com/10.1007/978-3-319-12643-2{\_}31},
year = {2014}
}
@article{Gudi2015,
abstract = {â€” Ground truth annotation of the occurrence and intensity of FACS Action Unit (AU) activation requires great amount of attention. The efforts towards achieving a common platform for AU evaluation have been addressed in the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015). Participants are invited to estimate AU occurrence and intensity on a common benchmark dataset. Conventional approaches towards achieving automated methods are to train multiclass classifiers or to use regression models. In this paper, we propose a novel application of a deep convolutional neural network (CNN) to recognize AUs as part of FERA 2015 challenge. The 7 layer network is composed of 3 convolutional layers and a max-pooling layer. The final fully connected layers provide the classification output. For the selected tasks of the challenge, we have trained two different networks for the two different datasets, where one focuses on the AU occurrences and the other on both occurrences and intensities of the AUs. The occurrence and intensity of AU activation are estimated using specific neuron activations of the output layer. This way, we are able to create a single network architecture that could simultaneously be trained to produce binary and continuous classification output.},
author = {Gudi, Amogh and Tasli, H Emrah and den Uyl, Tim M and Maroulis, Andreas},
file = {:vol/lm1015-tmp/mendeley/Gudi et al/2015/Gudi et al. - 2015 - Deep Learning based FACS Action Unit Occurrence and Intensity Estimation.pdf:pdf},
isbn = {9781479960262},
journal = {11th IEEE Int. Conf. Autom. Face Gesture Recognitions (FG 2015), FERA 2015 Chall.},
title = {{Deep Learning based FACS Action Unit Occurrence and Intensity Estimation}},
volume = {2013},
year = {2015}
}
@article{emonet,
abstract = {The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based "bag-of-mouths" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67{\%} on the 2014 dataset.},
archivePrefix = {arXiv},
arxivId = {1503.01800},
author = {Kahou, Samira Ebrahimi and Bouthillier, Xavier and Lamblin, Pascal and Gulcehre, Caglar and Michalski, Vincent and Konda, Kishore and Jean, S{\'{e}}bastien and Froumenty, Pierre and Dauphin, Yann and Boulanger-Lewandowski, Nicolas and Ferrari, Raul Chandias and Mirza, Mehdi and Warde-Farley, David and Courville, Aaron and Vincent, Pascal and Memisevic, Roland and Pal, Christopher and Bengio, Yoshua},
doi = {10.1051/0004-6361/201525993},
eprint = {1503.01800},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahou et al. - 2015 - EmoNets Multimodal deep learning approaches for emotion recognition in video.pdf:pdf},
journal = {Astroph},
keywords = {deep learning,emotion recognition,model combination,multimodal learning},
month = {mar},
number = {1},
pages = {15},
title = {{EmoNets: Multimodal deep learning approaches for emotion recognition in video}},
url = {http://arxiv.org/abs/1503.01800},
volume = {584},
year = {2015}
}
@article{Jaiswal2016,
abstract = {Spontaneous facial expression recognition under uncontrolled conditions is a hard task. It depends on multiple factors including shape, appearance and dynamics of the facial features, all of which are adversely affected by environmental noise and low intensity signals typical of such conditions. In this work, we present a novel approach to Facial Action Unit detection using a combination of Convolutional and Bi-directional Long Short-Term Memory Neural Networks (CNN-BLSTM), which jointly learns shape, appearance and dynamics in a deep learning manner. In addition, we introduce a novel way to encode shape features using binary image masks computed from the locations of facial landmarks. We show that the combination of dynamic CNN features and Bi-directional Long Short-Term Memory excels at modelling the temporal information. We thoroughly evaluate the contributions of each component in our system and show that it achieves state-of-the-art performance on the FERA-2015 Challenge dataset.},
author = {Jaiswal, Shashank and Valstar, Michel F.},
file = {:vol/lm1015-tmp/mendeley/Jaiswal, Valstar/2016/Jaiswal, Valstar - 2016 - Deep learning the dynamic appearance and shape of facial action units.pdf:pdf},
language = {en},
month = {jan},
title = {{Deep learning the dynamic appearance and shape of facial action units}},
url = {http://eprints.nottingham.ac.uk/31301/1/paper.pdf},
year = {2016}
}
@article{Kim2016,
author = {Kim, Bo-Kyeong and Roh, Jihyeon and Dong, Suh-Yeon and Lee, Soo-Young},
doi = {10.1007/s12193-015-0209-0},
file = {:vol/lm1015-tmp/mendeley/Kim et al/2016/Kim et al. - 2016 - Hierarchical committee of deep convolutional neural networks for robust facial expression recognition.pdf:pdf},
issn = {1783-7677},
journal = {J. Multimodal User Interfaces},
month = {jan},
title = {{Hierarchical committee of deep convolutional neural networks for robust facial expression recognition}},
url = {http://link.springer.com/10.1007/s12193-015-0209-0},
year = {2016}
}
@inproceedings{dodeeplearn,
abstract = {Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.},
archivePrefix = {arXiv},
arxivId = {1510.02969},
author = {Khorrami, Pooya and {Le Paine}, Tom and Huang, Thomas S.},
booktitle = {Proc. IEEE Int. Conf. Comput. Vis.},
doi = {10.1109/ICCVW.2015.12},
eprint = {1510.02969},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khorrami, Paine, Huang - 2015 - Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition.pdf:pdf},
isbn = {9781467383905},
issn = {15505499},
keywords = {Benchmark testing,Biological neural networks,Databases,Emotion recognition,Face,Face recognition,Training},
month = {oct},
pages = {19--27},
title = {{Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?}},
url = {http://arxiv.org/abs/1510.02969},
volume = {2016-Febru},
year = {2016}
}
@article{Khorrami2015,
abstract = {Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.},
archivePrefix = {arXiv},
arxivId = {1510.02969},
author = {Khorrami, Pooya and Paine, Tom Le and Huang, Thomas S.},
eprint = {1510.02969},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khorrami, Paine, Huang - 2015 - Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition.pdf:pdf},
month = {oct},
title = {{Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?}},
url = {http://arxiv.org/abs/1510.02969},
year = {2015}
}
@article{Ng2015,
author = {Ng, Hong-Wei and Nguyen, Viet Dung and Vonikakis, Vassilios and Winkler, Stefan},
doi = {10.1145/2818346.2830593},
file = {:vol/lm1015-tmp/mendeley/Ng et al/2015/Ng et al. - 2015 - Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning.pdf:pdf},
isbn = {978-1-4503-3912-4},
keywords = {deep learning networks,emotion classification,facial expression analysis},
month = {nov},
pages = {443--449},
publisher = {ACM},
title = {{Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning}},
url = {http://dl.acm.org/citation.cfm?id=2818346.2830593},
year = {2015}
}

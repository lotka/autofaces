Automatically generated by Mendeley Desktop 1.17-dev2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ng2015,
author = {Ng, Hong-Wei and Nguyen, Viet Dung and Vonikakis, Vassilios and Winkler, Stefan},
doi = {10.1145/2818346.2830593},
file = {:home/luka/Dropbox/Mendeley/Ng et al/2015/Ng et al. - 2015 - Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning.pdf:pdf},
isbn = {978-1-4503-3912-4},
keywords = {deep learning networks,emotion classification,facial expression analysis},
month = {nov},
pages = {443--449},
publisher = {ACM},
title = {{Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning}},
url = {http://dl.acm.org/citation.cfm?id=2818346.2830593},
year = {2015}
}
@article{Kim2016,
author = {Kim, Bo-Kyeong and Roh, Jihyeon and Dong, Suh-Yeon and Lee, Soo-Young},
doi = {10.1007/s12193-015-0209-0},
file = {:home/luka/Dropbox/Mendeley/Kim et al/2016/HIGH.pdf:pdf},
issn = {1783-7677},
journal = {J. Multimodal User Interfaces},
month = {jan},
title = {{Hierarchical committee of deep convolutional neural networks for robust facial expression recognition}},
url = {http://link.springer.com/10.1007/s12193-015-0209-0},
year = {2016}
}
@article{S.ZafeiriouA.PapaioannouI.KotsiaM.A.Nicolaou,
author = {{S. Zafeiriou, A. Papaioannou, I. Kotsia, M. A. Nicolaou}, G. Zhao.},
file = {:home/luka/Dropbox/Mendeley/S. Zafeiriou, A. Papaioannou, I. Kotsia, M. A. Nicolaou/2016/egpaper{\_}final{\_}1.pdf:pdf},
journal = {Int. Conf. Comput. Vis. Pattern Recognit. Work.},
title = {{Facial Affect “in-the-wild”: A survey and a new database}},
url = {http://ibug.doc.ic.ac.uk/media/uploads/documents/egpaper{\_}final{\_}1.pdf},
year = {2016}
}
@article{Parkhi2015,
abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M im- ages, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present meth- ods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/C.29.41},
eprint = {NIHMS150003},
file = {:home/luka/Dropbox/Mendeley/Parkhi, Vedaldi, Zisserman/2015/vgg.pdf:pdf},
isbn = {1-901725-53-7},
issn = {00313203},
journal = {Procedings Br. Mach. Vis. Conf. 2015},
number = {Section 3},
pages = {41.1--41.12},
pmid = {20847391},
title = {{Deep Face Recognition}},
url = {http://www.bmva.org/bmvc/2015/papers/paper041/index.html},
year = {2015}
}
@article{Gudi2015,
abstract = {— Ground truth annotation of the occurrence and intensity of FACS Action Unit (AU) activation requires great amount of attention. The efforts towards achieving a common platform for AU evaluation have been addressed in the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015). Participants are invited to estimate AU occurrence and intensity on a common benchmark dataset. Conventional approaches towards achieving automated methods are to train multiclass classifiers or to use regression models. In this paper, we propose a novel application of a deep convolutional neural network (CNN) to recognize AUs as part of FERA 2015 challenge. The 7 layer network is composed of 3 convolutional layers and a max-pooling layer. The final fully connected layers provide the classification output. For the selected tasks of the challenge, we have trained two different networks for the two different datasets, where one focuses on the AU occurrences and the other on both occurrences and intensities of the AUs. The occurrence and intensity of AU activation are estimated using specific neuron activations of the output layer. This way, we are able to create a single network architecture that could simultaneously be trained to produce binary and continuous classification output.},
author = {Gudi, Amogh and Tasli, H Emrah and den Uyl, Tim M and Maroulis, Andreas},
file = {:home/luka/Dropbox/Mendeley/Gudi et al/2015/07284873.pdf:pdf},
isbn = {9781479960262},
journal = {11th IEEE Int. Conf. Autom. Face Gesture Recognitions (FG 2015), FERA 2015 Chall.},
title = {{Deep Learning based FACS Action Unit Occurrence and Intensity Estimation}},
volume = {2013},
year = {2015}
}
@article{Jaiswal2016,
abstract = {Spontaneous facial expression recognition under uncontrolled conditions is a hard task. It depends on multiple factors including shape, appearance and dynamics of the facial features, all of which are adversely affected by environmental noise and low intensity signals typical of such conditions. In this work, we present a novel approach to Facial Action Unit detection using a combination of Convolutional and Bi-directional Long Short-Term Memory Neural Networks (CNN-BLSTM), which jointly learns shape, appearance and dynamics in a deep learning manner. In addition, we introduce a novel way to encode shape features using binary image masks computed from the locations of facial landmarks. We show that the combination of dynamic CNN features and Bi-directional Long Short-Term Memory excels at modelling the temporal information. We thoroughly evaluate the contributions of each component in our system and show that it achieves state-of-the-art performance on the FERA-2015 Challenge dataset.},
author = {Jaiswal, Shashank and Valstar, Michel F.},
file = {:home/luka/Dropbox/Mendeley/Jaiswal, Valstar/2016/paper.pdf:pdf},
language = {en},
month = {jan},
title = {{Deep learning the dynamic appearance and shape of facial action units}},
url = {http://eprints.nottingham.ac.uk/31301/1/paper.pdf},
year = {2016}
}
@article{S.EleftheriadisO.Rudovic,
author = {{S. Eleftheriadis, O. Rudovic}, M. Pantic},
file = {:home/luka/Dropbox/Mendeley/S. Eleftheriadis, O. Rudovic/2015/eleftheriadis{\_}iccv2015.pdf:pdf},
journal = {Int. Conf. Comput. Vis. (ICCV). Santiago, Chile},
title = {{Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection}},
url = {http://ibug.doc.ic.ac.uk/media/uploads/documents/eleftheriadis{\_}iccv2015.pdf},
year = {2015}
}
@article{tensorflow,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
annote = {Large-Scale Machine Learning on Heterogeneous Distributed Systems},
author = {{Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen}, Craig Citro and {Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat}, Ian Goodfellow and {Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz}, Lukasz Kaiser and {Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray}, ´ and {Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever}, Kunal Talwar and {Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals}, ´ and {Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu}, and Xiaoqiang Zheng},
file = {:home/luka/Dropbox/Mendeley/Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen et al/Unknown/whitepaper2015.pdf:pdf},
keywords = {Google,software,tensorflow,whitepaper},
mendeley-tags = {Google,software,tensorflow,whitepaper},
title = {{TensorFlow White Paper}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf}
}

Automatically generated by Mendeley Desktop 1.15.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Savran2012,
abstract = {a b s t r a c t Facial Action Coding System (FACS) is the de facto standard in the analysis of facial expressions. FACS de-scribes expressions in terms of the configuration and strength of atomic units called Action Units: AUs. FACS defines 44 AUs and each AU intensity is defined on a nonlinear scale of five grades. There has been sig-nificant progress in the literature on the detection of AUs. However, the companion problem of estimating the AU strengths has not been much investigated. In this work we propose a novel AU intensity estimation scheme applied to 2D luminance and/or 3D surface geometry images. Our scheme is based on regression of selected image features. These features are either non-specific, that is, those inherited from the AU detection algorithm, or are specific in that they are selected for the sole purpose of intensity estimation. For thorough-ness, various types of local 3D shape indicators have been considered, such as mean curvature, Gaussian cur-vature, shape index and curvedness, as well as their fusion. The feature selection from the initial plethora of Gabor moments is instrumented via a regression that optimizes the AU intensity predictions. Our AU inten-sity estimator is person-independent and when tested on 25 AUs that appear singly or in various combina-tions, it performs significantly better than the state-of-the-art method which is based on the margins of SVMs designed for AU detection. When evaluated comparatively, one can see that the 2D and 3D modalities have relative merits per upper face and lower face AUs, respectively, and that there is an overall improve-ment if 2D and 3D intensity estimations are used in fusion.},
author = {Savran, Arman and Sankur, Bulent and {Taha Bilge}, M},
doi = {10.1016/j.imavis.2011.11.008},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Savran, Sankur, Taha Bilge - 2012 - Regression-based intensity estimation of facial action units ☆.pdf:pdf},
journal = {IMAVIS},
keywords = {3D facial expression recognition,Action unit intensity estimation,AdaBoost.RT,Facial Action Coding System,Feature selection,SVM regression},
pages = {774--784},
title = {{Regression-based intensity estimation of facial action units ☆}},
volume = {30},
year = {2012}
}
@misc{AndrewGibiansky,
abstract = {Next, let's figure out how to do the exact same thing for convolutional neural networks. While the mathematical theory should be exactly the same, the actual derivation will be slightly more complex due to the architecture of convolutional neural networks.},
author = {{Andrew Gibiansky}},
booktitle = {2014},
title = {{Convolutional Neural Networks}},
url = {http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/},
urldate = {2016-06-10}
}
@article{Kim2016,
author = {Kim, Bo-Kyeong and Roh, Jihyeon and Dong, Suh-Yeon and Lee, Soo-Young},
doi = {10.1007/s12193-015-0209-0},
file = {:vol/lm1015-tmp/mendeley/Kim et al/2016/Kim et al. - 2016 - Hierarchical committee of deep convolutional neural networks for robust facial expression recognition.pdf:pdf},
issn = {1783-7677},
journal = {J. Multimodal User Interfaces},
month = {jan},
title = {{Hierarchical committee of deep convolutional neural networks for robust facial expression recognition}},
url = {http://link.springer.com/10.1007/s12193-015-0209-0},
year = {2016}
}
@book{ThomasDietterich,
abstract = {During the last years, semi-supervised learning has emerged as an exciting new direction in machine learning reseach. It is closely related to profound issues of how to do inference from data, as witnessed by its overlap with transductive inference (the distinctions are yet to be made precise).},
author = {Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
title = {{Semi-Supervised Learning}},
url = {http://www.acad.bg/ebook/ml/MITPress- SemiSupervised Learning.pdf}
}
@inproceedings{Ghosh2015,
author = {Ghosh, Sayan and Laksana, Eugene and Scherer, Stefan and Morency, Louis-Philippe},
booktitle = {2015 Int. Conf. Affect. Comput. Intell. Interact.},
doi = {10.1109/ACII.2015.7344632},
file = {:vol/lm1015-tmp/mendeley/Ghosh et al/2015/Ghosh et al. - 2015 - A multi-label convolutional neural network approach to cross-domain action unit detection.pdf:pdf},
isbn = {978-1-4799-9953-8},
keywords = {AU detection,Action Units,BP4D AU dataset,CK+ AU dataset,Convolutional Neural Networks,Cross-dataset transfer,DISFA AU dataset,Face recognition,Feature extraction,Gold,Neural networks,Testing,Training,Videos,affective computing,classification task,cross-domain action unit detection,face recognition,facial images,feature extractors,image classification,image representation,input image,learning (artificial intelligence),multilabel convolutional neural network approach,neural nets,shared representation learning,testing condition,training condition,visual databases},
language = {English},
month = {sep},
pages = {609--615},
publisher = {IEEE},
title = {{A multi-label convolutional neural network approach to cross-domain action unit detection}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7344632},
year = {2015}
}
@article{precogbook,
abstract = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics. An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
author = {{Duda O.}, Richard and {Hart E.}, Peter and {Stork G.}, David},
doi = {10.1007/BF01237942},
isbn = {978-0-471-05669-0},
issn = {0176-4268},
journal = {Zhurnal Eksp. i Teor. Fiz.},
pages = {680},
pmid = {2630878},
publisher = {Wiley},
title = {{Pattern Classification}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0$\backslash$nhttp://www.ai.mit.edu/courses/6.891-f00/text/DHSAppendix.pdf},
year = {2000}
}
@article{S.EleftheriadisO.Rudovic,
author = {{S. Eleftheriadis, O. Rudovic}, M. Pantic},
file = {:vol/lm1015-tmp/mendeley/S. Eleftheriadis, O. Rudovic/2015/S. Eleftheriadis, O. Rudovic - 2015 - Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection.pdf:pdf},
journal = {Int. Conf. Comput. Vis. (ICCV). Santiago, Chile},
title = {{Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection}},
url = {http://ibug.doc.ic.ac.uk/media/uploads/documents/eleftheriadis{\_}iccv2015.pdf},
year = {2015}
}
@article{Osendorfer2014,
abstract = {We present a computationally efficient architecture for image super-resolution that achieves state-of-the-art results on imageswith large spatial extend. Apart from utilizing Convolutional Neural Networks, our approach leverages recent advances in fast approximate inference for sparse coding.We empirically show that upsampling methods work much better on latent representations than in the original spatial domain. Our experiments indicate that the proposed architecture can serve as a basis for additional future improvements in image super-resolution.},
author = {Osendorfer, Christian and Soyer, Hubert and van der Smagt, Patrick},
doi = {10.1007/978-3-319-12643-2{\_}31},
file = {:vol/lm1015-tmp/mendeley/Osendorfer, Soyer, van der Smagt/2014/Osendorfer, Soyer, van der Smagt - 2014 - Image Super-Resolution with Fast Approximate Convolutional Sparse Coding.pdf:pdf},
isbn = {9783319126425},
issn = {16113349},
keywords = {convolutional neural,image processing,sparse coding},
pages = {250--257},
publisher = {Springer International Publishing},
title = {{Image Super-Resolution with Fast Approximate Convolutional Sparse Coding}},
url = {http://link.springer.com/10.1007/978-3-319-12643-2{\_}31},
year = {2014}
}
@article{Ng2015,
author = {Ng, Hong-Wei and Nguyen, Viet Dung and Vonikakis, Vassilios and Winkler, Stefan},
doi = {10.1145/2818346.2830593},
file = {:vol/lm1015-tmp/mendeley/Ng et al/2015/Ng et al. - 2015 - Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning.pdf:pdf},
isbn = {978-1-4503-3912-4},
keywords = {deep learning networks,emotion classification,facial expression analysis},
month = {nov},
pages = {443--449},
publisher = {ACM},
title = {{Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning}},
url = {http://dl.acm.org/citation.cfm?id=2818346.2830593},
year = {2015}
}
@inproceedings{W??llmer2012,
abstract = {Recent studies indicate that bidirectional Long Short-Term Memory (BLSTM) recurrent neural networks are well-suited for automatic emotion recognition systems and may lead to better results than systems applying other widely used classifiers such as Support Vector Machines or feedforward Neural Networks. The good performance of BLSTM emotion recognition systems could be attributed to their ability to model and exploit contextual information self-learned via recurrently connected memory blocks which allows them to incorporate information about how emotion evolves over time. However, the actual amount of bidirectional context that a BLSTM classifier takes into account when classifying an observation has not been investigated so far. This paper presents a methodology to systematically investigate the number of past and future utterance-level observations that are considered to generate an emotion prediction for a given utterance, and to examine to what extent this temporal bidirectional context contributes to the overall BLSTM performance. © 2012 IEEE.},
author = {W??llmer, Martin and Metallinou, Angeliki and Katsamanis, Nassos and Schuller, Bj??rn and Narayanan, Shrikanth},
booktitle = {ICASSP, IEEE Int. Conf. Acoust. Speech Signal Process. - Proc.},
doi = {10.1109/ICASSP.2012.6288834},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wollmer et al. - 2012 - Analyzing the memory of BLSTM Neural Networks for enhanced emotion classification in dyadic spoken interactions.pdf:pdf},
isbn = {9781467300469},
issn = {15206149},
keywords = {Long Short-Term Memory,context modeling,emotion recognition,sequential Jacobian},
month = {mar},
pages = {4157--4160},
publisher = {IEEE},
title = {{Analyzing the memory of BLSTM neural networks for enhanced emotion classification in dyadic spoken interactions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6288834},
year = {2012}
}
@article{disfa,
abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
doi = {10.1109/T-AFFC.2013.4},
file = {:vol/lm1015-tmp/mendeley/Mavadati et al/2013/Mavadati et al. - 2013 - DISFA A spontaneous facial action intensity database.pdf:pdf},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Trans. Affect. Comput.},
keywords = {FACS,action units,facial expression,intensity,spontaneous facial behavior,video corpus},
month = {apr},
number = {2},
pages = {151--160},
publisher = {IEEE},
title = {{DISFA: A spontaneous facial action intensity database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6475933},
volume = {4},
year = {2013}
}
@book{tfd,
author = {{J. M. Susskind, A. K. Anderson}, and G. E. Hinton},
publisher = {Department of Computer Science, University of Toronto, Toronto},
title = {{The Toronto Face Database}},
year = {2010}
}
@article{S.ZafeiriouA.PapaioannouI.KotsiaM.A.Nicolaou,
author = {{S. Zafeiriou, A. Papaioannou, I. Kotsia, M. A. Nicolaou}, G. Zhao.},
file = {:vol/lm1015-tmp/mendeley/S. Zafeiriou, A. Papaioannou, I. Kotsia, M. A. Nicolaou/2016/S. Zafeiriou, A. Papaioannou, I. Kotsia, M. A. Nicolaou - 2016 - Facial Affect “in-the-wild” A survey and a new database.pdf:pdf},
journal = {Int. Conf. Comput. Vis. Pattern Recognit. Work.},
title = {{Facial Affect “in-the-wild”: A survey and a new database}},
url = {http://ibug.doc.ic.ac.uk/media/uploads/documents/egpaper{\_}final{\_}1.pdf},
year = {2016}
}
@article{Eleftheriadis2016,
abstract = {We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation.},
archivePrefix = {arXiv},
arxivId = {1604.02917},
author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Deisenroth, Marc P. and Pantic, Maja},
eprint = {1604.02917},
file = {:vol/lm1015-tmp/mendeley/Eleftheriadis et al/2016/Eleftheriadis et al. - 2016 - Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis.pdf:pdf},
month = {apr},
title = {{Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis}},
url = {http://arxiv.org/abs/1604.02917},
year = {2016}
}
@article{Dosovitskiy2015,
abstract = {We describe an approach to incorporate diversity into spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the {\$}F{\_}1{\$} score of 90.18, and for German we achieve the {\$}F{\_}1{\$} score of 83.38.},
archivePrefix = {arXiv},
arxivId = {1506.0275},
author = {Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-10593-2{\_}13},
eprint = {1506.0275},
file = {:vol/lm1015-tmp/mendeley/Dosovitskiy, Brox/2015/Dosovitskiy, Brox - 2015 - Inverting Convolutional Networks with Convolutional Networks.pdf:pdf},
isbn = {9781941643327},
journal = {arXiv Prepr. arXiv1506.02753},
month = {jun},
pages = {1--15},
title = {{Inverting Convolutional Networks with Convolutional Networks}},
url = {http://arxiv.org/abs/1506.02753 http://arxiv.org/abs/1506.0275},
year = {2015}
}
@article{emonet,
abstract = {The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based "bag-of-mouths" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67{\%} on the 2014 dataset.},
archivePrefix = {arXiv},
arxivId = {1503.01800},
author = {Kahou, Samira Ebrahimi and Bouthillier, Xavier and Lamblin, Pascal and Gulcehre, Caglar and Michalski, Vincent and Konda, Kishore and Jean, S{\'{e}}bastien and Froumenty, Pierre and Dauphin, Yann and Boulanger-Lewandowski, Nicolas and Ferrari, Raul Chandias and Mirza, Mehdi and Warde-Farley, David and Courville, Aaron and Vincent, Pascal and Memisevic, Roland and Pal, Christopher and Bengio, Yoshua},
doi = {10.1051/0004-6361/201525993},
eprint = {1503.01800},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahou et al. - 2015 - EmoNets Multimodal deep learning approaches for emotion recognition in video.pdf:pdf},
journal = {Astroph},
keywords = {deep learning,emotion recognition,model combination,multimodal learning},
month = {mar},
number = {1},
pages = {15},
title = {{EmoNets: Multimodal deep learning approaches for emotion recognition in video}},
url = {http://arxiv.org/abs/1503.01800},
volume = {584},
year = {2015}
}
@article{tensorflow,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
annote = {Large-Scale Machine Learning on Heterogeneous Distributed Systems},
author = {{Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen}, Craig Citro and {Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat}, Ian Goodfellow and {Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz}, Lukasz Kaiser and {Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray}, ´ and {Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever}, Kunal Talwar and {Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals}, ´ and {Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu}, and Xiaoqiang Zheng},
file = {:vol/lm1015-tmp/mendeley/Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen et al/Unknown/Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen et al. - Unknown - TensorFlow White Paper.pdf:pdf},
keywords = {Google,software,tensorflow,whitepaper},
mendeley-tags = {Google,software,tensorflow,whitepaper},
title = {{TensorFlow White Paper}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf}
}
@article{semaine,
abstract = {SEMAINE has created a large audiovisual database as part of an iterative approach to building Sensitive Artificial Listener (SAL) agents that can engage a person in a sustained, emotionally coloured conversation. Data used to build the agents came from interactions between users and an 'operator' simulating a SAL agent, in different configurations: Solid SAL (designed so that operators displayed appropriate non-verbal behaviour) and Semi-automatic SAL (designed so that users' experience approximated interacting with a machine). We then recorded user interactions with the developed system, Automatic SAL, comparing the most communicatively competent version to versions with reduced nonverbal skills. High quality recording was provided by 5 high-resolution, high framerate cameras, and 4 microphones, recorded synchronously. Recordings total 150 participants, for a total of 959 conversations with individual SAL characters, lasting approximately 5 minutes each. Solid SAL recordings are transcribed and extensively annotated: 6-8 raters per clip traced five affective dimensions and 27 associated categories. Other scenarios are labelled on the same pattern, but less fully. Additional information includes FACS annotation on selected extracts, identification of laughs, nods and shakes, and measures of user engagement with the automatic system. The material is available through a web-accessible database.},
author = {McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schr{\"{o}}der, Marc},
doi = {10.1109/T-AFFC.2011.20},
file = {:vol/lm1015-tmp/mendeley/McKeown et al/2012/McKeown et al. - 2012 - The SEMAINE database Annotated multimodal records of emotionally colored conversations between a person and a li.pdf:pdf},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Trans. Affect. Comput.},
keywords = {Emotional corpora,affective annotation,affective computing,social signal processing},
number = {1},
pages = {5--17},
title = {{The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent}},
volume = {3},
year = {2012}
}
@article{Parkhi2015,
abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M im- ages, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present meth- ods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/C.29.41},
eprint = {NIHMS150003},
file = {:vol/lm1015-tmp/mendeley/Parkhi, Vedaldi, Zisserman/2015/Parkhi, Vedaldi, Zisserman - 2015 - Deep Face Recognition.pdf:pdf},
isbn = {1-901725-53-7},
issn = {00313203},
journal = {Procedings Br. Mach. Vis. Conf. 2015},
number = {Section 3},
pages = {41.1--41.12},
pmid = {20847391},
title = {{Deep Face Recognition}},
url = {http://www.bmva.org/bmvc/2015/papers/paper041/index.html},
year = {2015}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten-tive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin-guished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04623v1},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Wierstra, Daan},
eprint = {arXiv:1502.04623v1},
file = {:vol/lm1015-tmp/mendeley/Gregor et al/2014/Gregor et al. - 2014 - DRAW A Recurrent Neural Network For Image Generation.pdf:pdf},
month = {feb},
pages = {1--16},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2014}
}
@article{autong,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:vol/lm1015-tmp/mendeley/Ng/2011/Ng - 2011 - Sparse autoencoder.pdf:pdf},
institution = {Stanford},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lect. notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@incollection{JoshuaM.Susskind123,
abstract = {This book provides an overview of state of the art research in Affective Computing. It presents new ideas, original results and practical experiences in this increasingly important research field. The book consists of 23 chapters categorized into four sections. Since one of the most important means of human communication is facial expression, the first section of this book (Chapters 1 to 7) presents a research on synthesis and recognition of facial expressions. Given that we not only use the face but also body movements to express ourselves, in the second section (Chapters 8 to 11) we present a research on perception and generation of emotional expressions by using full-body motions. The third section of the book (Chapters 12 to 16) presents computational models on emotion, as well as findings from neuroscience research. In the last section of the book (Chapters 17 to 22) we present applications related to affective computing.},
author = {{Joshua M. Susskind}, Geoffrey E. Hinton and Anderson, Javier R. Movellan and Adam K.},
booktitle = {Affect. Comput.},
chapter = {23},
doi = {10.5772/56897},
editor = {Or, Jimmy},
file = {:vol/lm1015-tmp/mendeley/Joshua M. Susskind, Anderson/2008/Joshua M. Susskind, Anderson - 2008 - Generating Facial Expressions with Deep Belief Nets.pdf:pdf},
isbn = {978-3-902613-23-3},
pages = {452},
publisher = {I-Tech Education and Publishing},
title = {{Generating Facial Expressions with Deep Belief Nets}},
url = {http://www.cs.toronto.edu/{~}fritz/absps/joshfacechapter.pdf},
year = {2008}
}
@article{ThibaudSenechal2016,
author = {{Thibaud Senechal}, Daniel McDuff and Rana el Kaliouby},
file = {:vol/lm1015-tmp/mendeley/Thibaud Senechal/2016/Thibaud Senechal - 2016 - Facial Action Unit Detection using Active Learning and an Efficient Non-Linear Kernel Approximation.pdf:pdf},
journal = {Affectiva},
title = {{Facial Action Unit Detection using Active Learning and an Efficient Non-Linear Kernel Approximation}},
url = {http://www.affectiva.com/wp-content/uploads/2016/02/Facial-Action-Unit-Detection-using-Active-Learning-and-an-Efficient-Non-Linear-Kernel-Approximation.pdf},
year = {2016}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
file = {:vol/lm1015-tmp/mendeley/Kingma, Welling/2013/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
month = {dec},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@inproceedings{Taigman2014,
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
booktitle = {2014 IEEE Conf. Comput. Vis. Pattern Recognit.},
doi = {10.1109/CVPR.2014.220},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
isbn = {978-1-4799-5118-5},
keywords = {3D face modeling,Agriculture,DeepFace,Face,Face recognition,LFW dataset,Shape,Solid modeling,Three-dimensional displays,Training,alignment step,deep neural network,face recognition,face representation,face verification,human-level performance,image representation,labeled faces in the wild,neural nets,piecewise affine transformation,representation step},
month = {jun},
pages = {1701--1708},
publisher = {IEEE},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909616},
year = {2014}
}
@article{mnist,
abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.},
author = {LeCun, Y. and Cortes, C. and Burges, C. J. C.},
isbn = {7842500200015},
title = {{The MNIST database of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {1998}
}
@article{Amari1993,
abstract = {The backpropagation learning method has opened a way to wide applications of neural network research. It is a type of the stochastic descent method known in the sixties. The present paper reviews the wide applicability of the stochastic gradient descent method to various types of models and loss functions. In particular, we apply it to the pattern recognition problem, obtaining a new learning algorithm based on the information criterion. Dynamical properties of learning curves are then studied based on an old paper by the author where the stochastic descent method was proposed for general multilayer networks. The paper is concluded with a short section offering some historical remarks.},
author = {Amari, Shun-ichi},
doi = {10.1016/0925-2312(93)90006-O},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {dynamics of learning,generalized delta rule,multi-,pattern classification,stochastic descent},
number = {4-5},
pages = {185--196},
title = {{Backpropagation and stochastic gradient descent method}},
volume = {5},
year = {1993}
}
@article{stacks,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2010 - Stacked Denoising Autoencoders Learning Useful Representations in a Deep Network with a Local Denoising Criterio.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
number = {3},
pages = {3371--3408},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion}},
volume = {11},
year = {2010}
}
@article{Gudi2015,
abstract = {— Ground truth annotation of the occurrence and intensity of FACS Action Unit (AU) activation requires great amount of attention. The efforts towards achieving a common platform for AU evaluation have been addressed in the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015). Participants are invited to estimate AU occurrence and intensity on a common benchmark dataset. Conventional approaches towards achieving automated methods are to train multiclass classifiers or to use regression models. In this paper, we propose a novel application of a deep convolutional neural network (CNN) to recognize AUs as part of FERA 2015 challenge. The 7 layer network is composed of 3 convolutional layers and a max-pooling layer. The final fully connected layers provide the classification output. For the selected tasks of the challenge, we have trained two different networks for the two different datasets, where one focuses on the AU occurrences and the other on both occurrences and intensities of the AUs. The occurrence and intensity of AU activation are estimated using specific neuron activations of the output layer. This way, we are able to create a single network architecture that could simultaneously be trained to produce binary and continuous classification output.},
author = {Gudi, Amogh and Tasli, H Emrah and den Uyl, Tim M and Maroulis, Andreas},
file = {:vol/lm1015-tmp/mendeley/Gudi et al/2015/Gudi et al. - 2015 - Deep Learning based FACS Action Unit Occurrence and Intensity Estimation.pdf:pdf},
isbn = {9781479960262},
journal = {11th IEEE Int. Conf. Autom. Face Gesture Recognitions (FG 2015), FERA 2015 Chall.},
title = {{Deep Learning based FACS Action Unit Occurrence and Intensity Estimation}},
volume = {2013},
year = {2015}
}
@article{Jaiswal2016,
abstract = {Spontaneous facial expression recognition under uncontrolled conditions is a hard task. It depends on multiple factors including shape, appearance and dynamics of the facial features, all of which are adversely affected by environmental noise and low intensity signals typical of such conditions. In this work, we present a novel approach to Facial Action Unit detection using a combination of Convolutional and Bi-directional Long Short-Term Memory Neural Networks (CNN-BLSTM), which jointly learns shape, appearance and dynamics in a deep learning manner. In addition, we introduce a novel way to encode shape features using binary image masks computed from the locations of facial landmarks. We show that the combination of dynamic CNN features and Bi-directional Long Short-Term Memory excels at modelling the temporal information. We thoroughly evaluate the contributions of each component in our system and show that it achieves state-of-the-art performance on the FERA-2015 Challenge dataset.},
author = {Jaiswal, Shashank and Valstar, Michel F.},
file = {:vol/lm1015-tmp/mendeley/Jaiswal, Valstar/2016/Jaiswal, Valstar - 2016 - Deep learning the dynamic appearance and shape of facial action units.pdf:pdf},
language = {en},
month = {jan},
title = {{Deep learning the dynamic appearance and shape of facial action units}},
url = {http://eprints.nottingham.ac.uk/31301/1/paper.pdf},
year = {2016}
}
@inproceedings{dodeeplearn,
abstract = {Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.},
archivePrefix = {arXiv},
arxivId = {1510.02969},
author = {Khorrami, Pooya and {Le Paine}, Tom and Huang, Thomas S.},
booktitle = {Proc. IEEE Int. Conf. Comput. Vis.},
doi = {10.1109/ICCVW.2015.12},
eprint = {1510.02969},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khorrami, Paine, Huang - 2015 - Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition.pdf:pdf},
isbn = {9781467383905},
issn = {15505499},
keywords = {Benchmark testing,Biological neural networks,Databases,Emotion recognition,Face,Face recognition,Training},
month = {oct},
pages = {19--27},
title = {{Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?}},
url = {http://arxiv.org/abs/1510.02969},
volume = {2016-Febru},
year = {2016}
}
@article{Davis2006,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present re-sults for binary decision problems in ma-chine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has proper-ties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that opti-mize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
author = {Davis, Jesse and Goadrich, Mark},
doi = {10.1145/1143844.1143874},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - Unknown - The Relationship Between Precision-Recall and ROC Curves.pdf:pdf},
isbn = {1595933832},
issn = {14710080},
pmid = {19165215},
title = {{davisgoadrichcamera2.pdf (application/pdf Object)}},
url = {http://pages.cs.wisc.edu/{~}jdavis/davisgoadrichcamera2.pdf},
year = {2006}
}
@article{Corneanu2016,
abstract = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
author = {Corneanu, Ciprian A and Oliu, Marc and Cohn, Jeffrey F and Escalera, Sergio},
doi = {10.1109/TPAMI.2016.2515606},
issn = {1939-3539},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {jan},
pmid = {26761193},
title = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-related Applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26761193},
year = {2016}
}
@article{Glauner2015,
abstract = {This thesis describes the design and implementation of a smile detector based on deep convolutional neural networks. It starts with a summary of neural networks, the difficulties of training them and new training methods, such as Restricted Boltzmann Machines or autoencoders. It then provides a literature review of convolutional neural networks and recurrent neural networks. In order to select databases for smile recognition, comprehensive statistics of databases popular in the field of facial expression recognition were generated and are summarized in this thesis. It then proposes a model for smile detection, of which the main part is implemented. The experimental results are discussed in this thesis and justified based on a comprehensive model selection performed. All experiments were run on a Tesla K40c GPU benefiting from a speedup of up to factor 10 over the computations on a CPU. A smile detection test accuracy of 99.45{\%} is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches with accuracies ranging from 65.55{\%} to 79.67{\%}. This experiment is re-run under various variations, such as retaining less neutral images or only the low or high intensities, of which the results are extensively compared.},
archivePrefix = {arXiv},
arxivId = {1508.06535},
author = {Glauner, Patrick O.},
eprint = {1508.06535},
file = {:vol/lm1015-tmp/mendeley/Glauner/2015/Glauner - 2015 - Deep Convolutional Neural Networks for Smile Recognition.pdf:pdf},
month = {aug},
title = {{Deep Convolutional Neural Networks for Smile Recognition}},
url = {http://arxiv.org/abs/1508.06535},
year = {2015}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf:pdf},
isbn = {9781600066634},
journal = {Iclr},
month = {dec},
pages = {1--14},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2015}
}
@inproceedings{Valstar,
abstract = {Despite efforts towards evaluation standards in facial expression analysis (e.g. FERA 2011), there is a need for up-to-date standardised evaluation procedures, focusing in particular on current challenges in the field. One of the challenges that is actively being addressed is the automatic estimation of expression intensities. To continue to provide a standardisation platform and to help the field progress beyond its current limitations, the FG 2015 Facial Expression Recognition and Analysis challenge (FERA 2015) will challenge participants to estimate FACS Action Unit (AU) intensity as well as AU occurrence on a common benchmark dataset with reliable manual annotations. Evaluation will be done using a clear and well-defined protocol. In this paper we present the second such challenge in automatic recognition of facial expressions, to be held in conjunction with the 11 IEEE conference on Face and Gesture Recognition, May 2015, in Ljubljana, Slovenia. Three sub-challenges are defined: the detection of AU occurrence, the estimation of AU intensity for pre-segmented data, and fully automatic AU intensity estimation. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for the three sub-challenges.},
author = {Valstar, Michel F and Almaev, Timur and Girard, Jeffrey M and McKeown, Gary and Mehu, Marc and Yin, Lijun and Pantic, Maja and Cohn, Jeffrey F},
booktitle = {2015 11th IEEE Int. Conf. Work. Autom. Face Gesture Recognit.},
doi = {10.1109/FG.2015.7284874},
file = {:vol/lm1015-tmp/mendeley/Valstar et al/2015/Valstar et al. - 2015 - FERA 2015 - second Facial Expression Recognition and Analysis challenge.pdf:pdf},
isbn = {978-1-4799-6026-2},
keywords = {AU intensity,AU occurrence,Databases,Estimation,FACS action unit,FERA 2015,Face recognition,Feature extraction,Gold,Reliability,Training,emotion recognition,expression intensities estimation,face recognition,facial expression analysis,facial expression recognition and analysis challen,standardisation platform},
pages = {1--8},
title = {{FERA 2015 - second Facial Expression Recognition and Analysis challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7284874},
volume = {06},
year = {2015}
}
@article{Khorrami2015,
abstract = {Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.},
archivePrefix = {arXiv},
arxivId = {1510.02969},
author = {Khorrami, Pooya and Paine, Tom Le and Huang, Thomas S.},
eprint = {1510.02969},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khorrami, Paine, Huang - 2015 - Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition.pdf:pdf},
month = {oct},
title = {{Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?}},
url = {http://arxiv.org/abs/1510.02969},
year = {2015}
}
@article{Lucey2010,
abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this pe-riod, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algo-rithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22{\%} and the number of subjects by 27{\%}. The target expres-sion for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a lin-ear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
author = {Lucey, Patrick and Cohn, Jeffrey F and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
file = {:vol/lm1015-tmp/mendeley/Lucey et al/2010/Lucey et al. - 2010 - The extended cohn-kande dataset (CK) A complete facial expression dataset for action unit and emotionspeciﬁed expre.pdf:pdf},
isbn = {9781424470303},
journal = {IEEE Conf. Comput. Vis. Pattern Recognit. Work.},
number = {July},
pages = {94--101},
title = {{The extended cohn-kande dataset (CK+): A complete facial expression dataset for action unit and emotionspeciﬁed expression}},
year = {2010}
}
@article{Rocki2016,
abstract = {There exists a theory of a single general-purpose learning algorithm which could explain the principles of its operation. This theory assumes that the brain has some initial rough architecture, a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from both architectural and functional perspectives.},
archivePrefix = {arXiv},
arxivId = {1603.08262},
author = {Rocki, Kamil},
eprint = {1603.08262},
file = {:vol/lm1015-tmp/mendeley/Rocki/2016/Rocki - 2016 - Towards Machine Intelligence.pdf:pdf},
journal = {IBM},
month = {mar},
title = {{Towards Machine Intelligence}},
url = {http://arxiv.org/abs/1603.08262},
year = {2016}
}
@article{Goel2014,
abstract = {Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1412.6093},
author = {Goel, Kratarth and Vohra, Raunaq},
eprint = {1412.6093},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goel, Vohra - 2014 - Learning Temporal Dependencies in Data Using a DBN-BLSTM.pdf:pdf},
month = {dec},
pages = {6},
title = {{Learning Temporal Dependencies in Data Using a DBN-BLSTM}},
url = {http://arxiv.org/abs/1412.6093},
year = {2014}
}
@article{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:homes/lm1015/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}

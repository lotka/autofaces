Automatically generated by Mendeley Desktop 1.15.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Amari1993,
abstract = {The backpropagation learning method has opened a way to wide applications of neural network research. It is a type of the stochastic descent method known in the sixties. The present paper reviews the wide applicability of the stochastic gradient descent method to various types of models and loss functions. In particular, we apply it to the pattern recognition problem, obtaining a new learning algorithm based on the information criterion. Dynamical properties of learning curves are then studied based on an old paper by the author where the stochastic descent method was proposed for general multilayer networks. The paper is concluded with a short section offering some historical remarks.},
author = {Amari, Shun-ichi},
doi = {10.1016/0925-2312(93)90006-O},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {dynamics of learning,generalized delta rule,multi-,pattern classification,stochastic descent},
number = {4-5},
pages = {185--196},
title = {{Backpropagation and stochastic gradient descent method}},
volume = {5},
year = {1993}
}
@article{Lucey2010,
abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this pe-riod, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algo-rithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22{\%} and the number of subjects by 27{\%}. The target expres-sion for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a lin-ear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
author = {Lucey, Patrick and Cohn, Jeffrey F and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
file = {:vol/lm1015-tmp/mendeley/Lucey et al/2010/Lucey et al. - 2010 - The extended cohn-kande dataset (CK) A complete facial expression dataset for action unit and emotionspeciﬁed expre.pdf:pdf},
isbn = {9781424470303},
journal = {IEEE Conf. Comput. Vis. Pattern Recognit. Work.},
number = {July},
pages = {94--101},
title = {{The extended cohn-kande dataset (CK+): A complete facial expression dataset for action unit and emotionspeciﬁed expression}},
year = {2010}
}
@article{mnist,
abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.},
author = {LeCun, Y. and Cortes, C. and Burges, C. J. C.},
isbn = {7842500200015},
title = {{The MNIST database of handwritten digits}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {1998}
}
@article{disfa,
abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
doi = {10.1109/T-AFFC.2013.4},
file = {:vol/lm1015-tmp/mendeley/Mavadati et al/2013/Mavadati et al. - 2013 - DISFA A spontaneous facial action intensity database.pdf:pdf},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Trans. Affect. Comput.},
keywords = {FACS,action units,facial expression,intensity,spontaneous facial behavior,video corpus},
month = {apr},
number = {2},
pages = {151--160},
publisher = {IEEE},
title = {{DISFA: A spontaneous facial action intensity database}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6475933},
volume = {4},
year = {2013}
}
@article{semaine,
abstract = {SEMAINE has created a large audiovisual database as part of an iterative approach to building Sensitive Artificial Listener (SAL) agents that can engage a person in a sustained, emotionally coloured conversation. Data used to build the agents came from interactions between users and an 'operator' simulating a SAL agent, in different configurations: Solid SAL (designed so that operators displayed appropriate non-verbal behaviour) and Semi-automatic SAL (designed so that users' experience approximated interacting with a machine). We then recorded user interactions with the developed system, Automatic SAL, comparing the most communicatively competent version to versions with reduced nonverbal skills. High quality recording was provided by 5 high-resolution, high framerate cameras, and 4 microphones, recorded synchronously. Recordings total 150 participants, for a total of 959 conversations with individual SAL characters, lasting approximately 5 minutes each. Solid SAL recordings are transcribed and extensively annotated: 6-8 raters per clip traced five affective dimensions and 27 associated categories. Other scenarios are labelled on the same pattern, but less fully. Additional information includes FACS annotation on selected extracts, identification of laughs, nods and shakes, and measures of user engagement with the automatic system. The material is available through a web-accessible database.},
author = {McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schr{\"{o}}der, Marc},
doi = {10.1109/T-AFFC.2011.20},
file = {:vol/lm1015-tmp/mendeley/McKeown et al/2012/McKeown et al. - 2012 - The SEMAINE database Annotated multimodal records of emotionally colored conversations between a person and a li.pdf:pdf},
isbn = {1949-3045},
issn = {19493045},
journal = {IEEE Trans. Affect. Comput.},
keywords = {Emotional corpora,affective annotation,affective computing,social signal processing},
number = {1},
pages = {5--17},
title = {{The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent}},
volume = {3},
year = {2012}
}

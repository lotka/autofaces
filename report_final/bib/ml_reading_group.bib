Automatically generated by Mendeley Desktop 1.16.2-dev5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Santoro2016,
abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
archivePrefix = {arXiv},
arxivId = {1605.06065},
author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
eprint = {1605.06065},
file = {:home/luka/Documents/Mendeley/Santoro et al/Santoro et al. - 2016 - One-shot Learning with Memory-Augmented Neural Networks.pdf:pdf},
month = {may},
pages = {13},
title = {{One-shot Learning with Memory-Augmented Neural Networks}},
url = {http://arxiv.org/abs/1605.06065},
year = {2016}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {1308.0850},
author = {Graves, Alex},
eprint = {1308.0850},
file = {:home/luka/Documents/Mendeley/Graves/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
month = {aug},
title = {{Generating Sequences With Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Graves2016,
abstract = {This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.},
archivePrefix = {arXiv},
arxivId = {1603.08983},
author = {Graves, Alex},
eprint = {1603.08983},
file = {:home/luka/Documents/Mendeley/Graves/Graves - 2016 - Adaptive Computation Time for Recurrent Neural Networks.pdf:pdf},
month = {mar},
title = {{Adaptive Computation Time for Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1603.08983},
year = {2016}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:home/luka/Documents/Mendeley/Graves, Wayne, Danihelka/Graves, Wayne, Danihelka - 2014 - Neural Turing Machines.pdf:pdf},
month = {oct},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@inproceedings{Grefenstette2015,
author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
booktitle = {Adv. Neural Inf. Process. Syst.},
file = {:home/luka/Documents/Mendeley/Grefenstette et al/Grefenstette et al. - 2015 - Learning to Transduce with Unbounded Memory.pdf:pdf},
pages = {1828--1836},
title = {{Learning to Transduce with Unbounded Memory}},
url = {http://papers.nips.cc/paper/5648-self-paced-learning-with-diversity},
year = {2015}
}
